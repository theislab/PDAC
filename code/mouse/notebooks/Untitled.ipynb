{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39d441ad-f03b-4e35-8a8a-841c47ca08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3672fc31-e2f1-4061-85b1-33661bee477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/storage/Daniele/preprocessed_data/cellxgene_census_human/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40fcd915-f74f-457c-9593-eea6f9ad3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_m = '/mnt/storage/Daniele/preprocessed_data/cellxgene_census_mouse/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99bfe33e-ca2a-4fab-8ea3-841f4d3f7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = sc.read_h5ad(f'{path}0_preprocessed.h5ad', backed = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee5f79b8-b74e-4332-9943-db1c1f313bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_m = sc.read_h5ad(f'{path_m}0_preprocessed.h5ad', backed = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95bf752a-5077-483e-ac62-36796c93749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc50f7-f524-4fd4-9691-80fcb70e0bb5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Old versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6de84490-c0d2-4a09-b58d-703907d7d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniele/miniconda3/envs/scmouse_atlas/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "from datasets import Dataset, Features, Sequence, Value, concatenate_datasets, load_from_disk\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class AnnDataHFConverter:\n",
    "    \"\"\"\n",
    "    A class to convert AnnData objects to Hugging Face datasets with support for batching and parallel processing.\n",
    "\n",
    "    Attributes:\n",
    "        adata (AnnData): The AnnData object containing the single-cell dataset to be converted.\n",
    "        batch_size (int, optional): The size of each batch to process. Defaults to the number of observations in `adata`.\n",
    "        gene_tokens (str): Column name in `adata.var` representing gene tokens.\n",
    "        columns (List[str], optional): Additional observation metadata columns to include in the dataset.\n",
    "        n_workers (int, optional): The number of worker processes for parallel computation. Defaults to 1 (sequential processing).\n",
    "\n",
    "    Methods:\n",
    "        generate_datasets(save: Optional[str] = None, merge: Optional[bool] = False): Converts `adata` into one or more Hugging Face datasets.\n",
    "        _prepare_features(): Prepares feature descriptions for the Hugging Face dataset based on `adata` structure.\n",
    "        _adata_to_dataset(adata_batch: AnnData, features: Features): Converts a single batch of AnnData to a Hugging Face Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, adata: AnnData, columns: List[str] = ['total_counts', 'n_genes_by_counts'], batch_size: Optional[int] = None, n_workers: Optional[int] = 1, save: Optional[str] = None, merge: Optional[bool] = True):\n",
    "        \"\"\"\n",
    "        Initializes the AnnDataHFConverter_parallel object with the specified AnnData, batch size, gene tokens, columns, and number of workers.\n",
    "\n",
    "        Parameters:\n",
    "            columns (List[str]): Additional columns from `adata.obs` to be included in the datasets.\n",
    "            batch_size (int, optional): Number of observations to process per batch. If None, the whole dataset is processed as one batch.\n",
    "            n_workers (int, optional): Number of parallel worker processes to use. Defaults to 1 for sequential processing.\n",
    "        \"\"\"\n",
    "\n",
    "        self.adata = adata\n",
    "        self.columns = columns\n",
    "        self.batch_size = batch_size if batch_size is not None else self.adata.n_obs\n",
    "        self.n_workers = n_workers\n",
    "        self.save = save\n",
    "        self.merge = merge\n",
    "\n",
    "    def generate_datasets(self) -> Optional[Union[Dataset, List[Dataset]]]:\n",
    "        \"\"\"\n",
    "        Generates and optionally saves and/or merges datasets from the AnnData object.\n",
    "\n",
    "        Parameters:\n",
    "            save (str, optional): Path to save individual batches as separate datasets. If not specified, datasets are not saved.\n",
    "            merge (bool, optional): Whether to merge all batch datasets into a single dataset. If True and `save` is specified, the merged dataset is saved.\n",
    "\n",
    "        Returns:\n",
    "            Union[Dataset, List[Dataset], None]: The resulting dataset(s) if not saved, or None if saved to disk.\n",
    "        \"\"\"\n",
    "        \n",
    "        features = self._prepare_features()\n",
    "        if save:\n",
    "            save_path = Path(save)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        datasets = []\n",
    "\n",
    "        def process_batch(idx, start_idx, end_idx):\n",
    "            adata_batch = self.adata[start_idx:end_idx]\n",
    "            dataset = self._adata_to_HF(adata_batch, features)\n",
    "            if save:\n",
    "                batch_path = save_path / f\"{idx}.dataset\"\n",
    "                dataset.save_to_disk(str(batch_path))\n",
    "            del adata_batch\n",
    "            gc.collect()\n",
    "            return dataset\n",
    "\n",
    "        adata_splits = [(idx, start_idx, min(start_idx + self.batch_size, self.adata.n_obs)) for idx, start_idx in enumerate(range(0, self.adata.n_obs, self.batch_size))]\n",
    "        \n",
    "        if self.n_workers > 1:\n",
    "            datasets = Parallel(n_jobs=self.n_workers)(delayed(process_batch)(idx, start, end) for idx, start, end in adata_splits)\n",
    "        else:\n",
    "            for idx, start, end in adata_splits:\n",
    "                datasets.append(process_batch(idx, start, end))\n",
    "\n",
    "        if merge:\n",
    "            if save:\n",
    "                batches = list(save_path.iterdir())\n",
    "                batches_datasets = [load_from_disk(str(save_path / batch)) for batch in sorted(batches)]\n",
    "                dataset = concatenate_datasets(batches_datasets)\n",
    "                merged_path = save_path / \"merged.dataset\"\n",
    "                dataset.save_to_disk(str(merged_path))\n",
    "                return None\n",
    "            else:\n",
    "                dataset = concatenate_datasets(datasets)\n",
    "                return dataset\n",
    "        elif not save:\n",
    "            return datasets\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _prepare_features(self) -> Features:\n",
    "        \"\"\"\n",
    "        Prepares a Features object for the Hugging Face dataset based on the columns specified in the AnnData object.\n",
    "\n",
    "        Returns:\n",
    "            Features: A dictionary of features with data types corresponding to the columns in the AnnData object.\n",
    "        \"\"\"\n",
    "        features = {'gexp': Sequence(Value(\"float32\")), 'protein_embeddings': Sequence(Value(\"int32\")), 'gene_name': Sequence(Value(\"string\"))}\n",
    "        if self.columns:\n",
    "            for col in self.columns:\n",
    "                if self.adata.obs[col].dtype.name in ['category', 'category_']:\n",
    "                    features[col] = Value(\"string\")\n",
    "                else:\n",
    "                    features[col] = Value(str(self.adata.obs[col].dtype))\n",
    "        return Features(features)\n",
    "\n",
    "    def _adata_to_HF(self, adata_batch: AnnData, features: Features) -> Dataset:\n",
    "        \"\"\"\n",
    "        Converts a single batch of AnnData to a Hugging Face Dataset, more efficiently.\n",
    "\n",
    "        Parameters:\n",
    "            adata_batch (AnnData): A batch slice of the main AnnData object.\n",
    "            features (Features): The features for the dataset as prepared by `_prepare_features`.\n",
    "\n",
    "        Returns:\n",
    "            Dataset: A single batch converted into a Hugging Face Dataset.\n",
    "        \"\"\"\n",
    "        obs = adata_batch.obs\n",
    "        gexp = adata_batch.X.toarray() if not isinstance(adata_batch.X, np.ndarray) else adata_batch.X\n",
    "        gene_tokens = adata_batch.var[self.gene_tokens].to_numpy().astype(np.int32)\n",
    "        gene_names = adata_batch.var_names.to_numpy()\n",
    "        data_dict = {\n",
    "            'gexp': [gexp[cell] for cell in range(adata_batch.n_obs)],\n",
    "            'gene_token': [gene_tokens for _ in range(adata_batch.n_obs)],\n",
    "            'gene_name': [gene_names for _ in range(adata_batch.n_obs)]\n",
    "        }\n",
    "        if self.columns:\n",
    "            for col in self.columns:\n",
    "                data_dict[col] = adata_batch.obs[col].to_numpy()\n",
    "\n",
    "        dataset = Dataset.from_dict(data_dict, features=features)\n",
    "        dataset.set_format(type=\"torch\")\n",
    "\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626bb3e-311a-4b71-958b-ff4d184de26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix, hstack, issparse\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class AnnDataProcessor:\n",
    "    def __init__(self, adata: AnnData, protein_embeddings: Path, var_names: Optional[str] = None, isMouse: Optional[bool] = False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the processor with an AnnData object, specifying options for gene identifiers and QC metrics.\n",
    "\n",
    "        Args:\n",
    "            adata (AnnData): The AnnData object to be processed.\n",
    "            protein_embeddings (Path): Path to the file containing protein embeddings.\n",
    "            var_names (Optional[str]): Column in `adata.var` to use for gene identifiers, defaults to `adata.var_names` if None.\n",
    "            isMouse (Optional[bool]): Set to True if the dataset originates from mice, influencing the loaded gene vocabularies.\n",
    "        \"\"\"\n",
    "        logging.info(\"Initializing AnnDataProcessor\")\n",
    "        #self.genes_dictionary = self._load_genes_dictionary('mouse_gene_vocab.json' if isMouse else 'human_gene_vocab.json')\n",
    "        self.genes_dictionary = human_dict\n",
    "        self.protein_coding_genes = set(self.genes_dictionary.keys())\n",
    "        self.adata = adata\n",
    "        self.protein_embeddings = protein_embeddings\n",
    "        self.adata.var_names = self.adata.var[var_names] if var_names else self.adata.var_names\n",
    "        self.var_names = var_names\n",
    "        self.computeQC = computeQC\n",
    "        logging.info(\"AnnDataProcessor initialized successfully\")\n",
    "\n",
    "    def process_adata(self) -> None:\n",
    "        \"\"\"\n",
    "        Processes the AnnData object by ensuring all required genes are present, and computing QC metrics if specified.\n",
    "        \"\"\"\n",
    "        logging.info(\"Processing AnnData object\")\n",
    "        missing_genes = self.protein_coding_genes - set(self.adata.var_names)\n",
    "        if missing_genes:\n",
    "            logging.info(f\"Extending AnnData with {len(missing_genes)} missing genes\")\n",
    "            self._extend_anndata(list(missing_genes))\n",
    "        self.adata = self.adata[:, self.adata.var_names.isin(self.protein_coding_genes)].copy()\n",
    "        self.adata.var['feature_id'] = [self.genes_dictionary[gene][1] for gene in self.adata.var_names]\n",
    "        self.adata.var['feature_name'] = self.adata.var_names\n",
    "        self.adata.var = self.adata.var[['feature_id', 'feature_name', 'extended']]\n",
    "        logging.info(\"AnnData processing complete\")\n",
    "        logging.info(\"Loading protein embeddings\")\n",
    "        self.protein_embeddings = pd.read_parquet(self.protein_embeddings).loc[self.adata.var_names.values]\n",
    "\n",
    "    def _load_genes_dictionary(self, filename: str) -> Dict[str, List[Union[int, str]]]:\n",
    "        \"\"\"\n",
    "        Load a JSON file containing gene information and return it as a dictionary.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Filename of the JSON to load, relative to a predefined directory.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Union[int, str]]]: Dictionary containing gene information.\n",
    "        \"\"\"\n",
    "        relative_path = Path(__file__).parent / 'resources' / filename\n",
    "        with open(relative_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _extend_anndata(self, missing_genes: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Extend the AnnData object by adding missing genes. Adds these genes to the var DataFrame and sets their expression values to -1 (masked).\n",
    "\n",
    "        Args:\n",
    "            missing_genes (List[str]): List of gene names that are missing and need to be added.\n",
    "        \"\"\"\n",
    "        logging.info(f\"Adding {len(missing_genes)} missing genes to AnnData\")\n",
    "        new_var = pd.DataFrame(index=missing_genes)\n",
    "        new_var['extended'] = True  # Mark these genes as extended\n",
    "        if not issparse(self.adata.X):\n",
    "            self.adata.X = csr_matrix(self.adata.X)\n",
    "        masked_matrix = csr_matrix((self.adata.n_obs, len(missing_genes)), dtype=float)\n",
    "        new_X = hstack([self.adata.X, masked_matrix]) \n",
    "        extended_var = pd.concat([self.adata.var, new_var], axis=0)\n",
    "        extended_var['extended'] = extended_var['extended'].fillna(value = False)\n",
    "\n",
    "        self.adata = AnnData(\n",
    "            X=new_X,\n",
    "            obs=self.adata.obs,\n",
    "            var=extended_var\n",
    "        )\n",
    "        logging.info(\"AnnData extension complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedecc7e-106f-4b2a-a7c4-e9cd20fbd596",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Old try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52835712-ecf7-4a3d-818e-f0a264cd397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from importlib import resources\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix, hstack, issparse\n",
    "\n",
    "\n",
    "class SetupAnnData:\n",
    "    def __init__(self, adata: AnnData, var_names: Optional[str] = None, isMouse: Optional[bool] = False, computeQC: Optional[bool] = False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the modifier with an AnnData object. Load default vocabularies based on the species.\n",
    "\n",
    "        Parameters:\n",
    "        adata (AnnData): The AnnData object to modify.\n",
    "        var_names (Optional[str]): Column in adata.var to use for gene identifiers, defaults to adata.var_names if None.\n",
    "        isMouse (Optional[bool]): Indicates if the dataset is from mouse, affects loaded vocabularies.\n",
    "        computeQC (Optional[bool]): If True, computes QC metrics and adds them to adata.\n",
    "        \"\"\"\n",
    "        self.vocab = h_d#self.load_json('human_gene_vocab.json') if not isMouse else self.load_json('mouse_gene_vocab.json')\n",
    "        self.gene_tokens_vocab = {k: v[0] for k, v in self.vocab.items()}\n",
    "        self.gene_ensembl_vocab = {k: v[1] for k, v in self.vocab.items()}\n",
    "        self._adata = adata.copy()\n",
    "        self._adata.var_names = self._adata.var_names if var_names else self._adata.var_names\n",
    "        if computeQC:\n",
    "            sc.pp.calculate_qc_metrics(self._adata, inplace=True)\n",
    "\n",
    "    def load_json(self, vocab_name: str) -> Dict[str, List[Union[int,str]]]:\n",
    "        \"\"\"\n",
    "        Load a JSON file from the specified relative path and return it as a dictionary.\n",
    "\n",
    "        Parameters:\n",
    "        filename (str): The filename of the JSON file to load relative to the class file location.\n",
    "\n",
    "        Returns:\n",
    "        Dict[str, List[int,str]]: The loaded JSON data.\n",
    "        \"\"\"\n",
    "        relative_path = Path(__file__).parent.parent / 'resources' / vocab_name\n",
    "        with open(relative_path) as file:\n",
    "            data = json.load(file)\n",
    "        if not isinstance(data, dict) or not all(isinstance(k, str) and isinstance(v, list) and isinstance(v[0], int) and isinstance(v[1], str) for k, v in data.items()):\n",
    "            raise ValueError(\"Format as expected: Dict[str, List[str]]\")  # noqa: TRY003\n",
    "        return data\n",
    "\n",
    "    def map_gene_ids(self) -> None:\n",
    "        \"\"\"\n",
    "        Update the AnnData object's var DataFrame based on the loaded JSON mapping for gene tokens and Ensembl IDs.\n",
    "        \"\"\"\n",
    "        gene_ids_mapping = pd.Series(self.gene_tokens_vocab)\n",
    "        gene_ensembl_mapping = pd.Series(self.gene_ensembl_vocab)\n",
    "        target_names = self._adata.var_names.astype(str)\n",
    "        self._adata.var['gene_token'] = target_names.map(gene_ids_mapping)\n",
    "        self._adata.var['gene_ensembl'] = target_names.map(gene_ensembl_mapping)\n",
    "\n",
    "    def extend_anndata(self, missing_genes: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Extend the AnnData object by adding missing genes. Adds these genes to the var DataFrame and sets their expression values to -1 (masked).\n",
    "        \"\"\"\n",
    "        new_var = pd.DataFrame(index=missing_genes)\n",
    "        masked_matrix = csr_matrix(np.full((self._adata.n_obs, len(missing_genes)), -1))  # Fill matrix with -1, mask on the flight missing genes\n",
    "        current_X = self._adata.X.tocsr() if issparse(self._adata.X) else csr_matrix(self._adata.X)\n",
    "        new_X = hstack([current_X, masked_matrix]).tocsr()\n",
    "        self._adata = AnnData(\n",
    "            X=new_X,\n",
    "            obs=self._adata.obs,\n",
    "            var=pd.concat([self._adata.var, new_var], axis=0)\n",
    "        )\n",
    "        self._adata.var_names_make_unique()\n",
    "\n",
    "    def process_adata(self) -> AnnData:\n",
    "        \"\"\"\n",
    "        Return the modified AnnData object after ensuring all genes in the vocabulary are present and mapped.\n",
    "        \"\"\"\n",
    "        adata_genes = set(self._adata.var_names)\n",
    "        vocab_genes = set(self.gene_tokens_vocab.keys())\n",
    "        missing_genes = vocab_genes - adata_genes\n",
    "        if missing_genes:\n",
    "            self.extend_anndata(list(missing_genes))\n",
    "        self._adata = self._adata[:,list(vocab_genes)].copy()\n",
    "        self.map_gene_ids()\n",
    "\n",
    "        return self._adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9527cae-6329-48b5-a5b9-9b7d1957bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "\n",
    "from datasets import (  # type: ignore  # noqa: PGH003\n",
    "    Dataset,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    concatenate_datasets,\n",
    "    load_from_disk,\n",
    ")\n",
    "\n",
    "\n",
    "class AnnDataHFConverter:\n",
    "    def __init__(self, adata: AnnData, batch_size: Optional[int] = None, gene_tokens: str = 'gene_token', columns: Optional[List[str]] = ['total_counts', 'n_genes_by_counts']) -> None:\n",
    "        self.adata = adata\n",
    "        self.batch_size = batch_size if batch_size is not None else self.adata.n_obs\n",
    "        self.gene_tokens = gene_tokens\n",
    "        self.columns = columns\n",
    "\n",
    "    def generate_datasets(self, save: Optional[str] = None, merge: Optional[bool] = False) -> Optional[Union[Dataset, List[Dataset]]]:\n",
    "        features = self._prepare_features()\n",
    "        total_cells = self.adata.n_obs\n",
    "\n",
    "        if save:\n",
    "            save_path = Path(save)\n",
    "            if not save_path.exists():\n",
    "                save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if self.batch_size == total_cells and merge:\n",
    "            merge = False\n",
    "\n",
    "        datasets = []\n",
    "        for idx, start_idx in enumerate(range(0, total_cells, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, total_cells)\n",
    "            adata_batch = self.adata[start_idx:end_idx].copy()\n",
    "            dataset = self._adata_to_dataset(adata_batch, features)\n",
    "            if save:\n",
    "                batch_path = save_path / f\"{idx}.dataset\"\n",
    "                dataset.save_to_disk(str(batch_path))\n",
    "            datasets.append(dataset)\n",
    "            del adata_batch\n",
    "            gc.collect()\n",
    "\n",
    "        if merge:\n",
    "            if save:\n",
    "                batches = list(save_path.iterdir())\n",
    "                batches_datasets = [load_from_disk(str(save_path / batch)) for batch in sorted(batches)]\n",
    "                dataset = concatenate_datasets(batches_datasets)\n",
    "                merged_path = save_path / \"merged.dataset\"\n",
    "                dataset.save_to_disk(str(merged_path))\n",
    "                return None\n",
    "            else:\n",
    "                dataset = concatenate_datasets(datasets)\n",
    "                return dataset\n",
    "        elif not save:\n",
    "            return datasets\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _prepare_features(self) -> Features:\n",
    "        features = {'gexp': Sequence(Value(\"float32\")), 'gene_token': Sequence(Value(\"int32\")), 'gene_name': Sequence(Value(\"string\"))}\n",
    "        if self.columns:\n",
    "            for col in self.columns:\n",
    "                if self.adata.obs[col].dtype.name in ['category', 'category_']:\n",
    "                    features[col] = Value(\"string\")\n",
    "                else:\n",
    "                    features[col] = Value(str(self.adata.obs[col].dtype))\n",
    "        return Features(features)\n",
    "\n",
    "\n",
    "    def _adata_to_dataset(self, adata_batch: AnnData, features: Features) -> Dataset:\n",
    "        \"\"\"\n",
    "        Converts a single batch of AnnData to a Hugging Face Dataset, more efficiently.\n",
    "\n",
    "        Parameters:\n",
    "        adata_batch (AnnData): A batch slice of the main AnnData object.\n",
    "\n",
    "        Returns:\n",
    "        Dataset: A single batch converted into a Hugging Face Dataset.\n",
    "        \"\"\"\n",
    "        obs = adata_batch.obs\n",
    "        gexp = adata_batch.X.toarray() if not isinstance(adata_batch.X, np.ndarray) else adata_batch.X\n",
    "        gene_tokens = adata_batch.var[self.gene_tokens].to_numpy().astype(np.int32)\n",
    "        gene_names = adata_batch.var_names.to_numpy()\n",
    "\n",
    "        data_dict = {\n",
    "            'gexp': [gexp[i].squeeze() for i in range(adata_batch.n_obs)],\n",
    "            'gene_token': [gene_tokens for _ in range(adata_batch.n_obs)],\n",
    "            'gene_name': [gene_names for _ in range(adata_batch.n_obs)]\n",
    "        }\n",
    "\n",
    "        if self.columns:\n",
    "            for col in self.columns:\n",
    "                data_dict[col] = obs[col].to_numpy()\n",
    "\n",
    "        dataset = Dataset.from_dict(data_dict, features=features)\n",
    "        dataset.set_format(type=\"torch\")\n",
    "\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a643fac-a5a2-44f1-a99b-b65ac402f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "from datasets import Dataset, Features, Sequence, Value, concatenate_datasets, load_from_disk\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class AnnDataHFConverter_parallel:\n",
    "    \"\"\"\n",
    "    A class to convert AnnData objects to Hugging Face datasets with support for batching and parallel processing.\n",
    "\n",
    "    Attributes:\n",
    "        adata (AnnData): The AnnData object containing the single-cell dataset to be converted.\n",
    "        batch_size (int, optional): The size of each batch to process. Defaults to the number of observations in `adata`.\n",
    "        gene_tokens (str): Column name in `adata.var` representing gene tokens.\n",
    "        columns (List[str], optional): Additional observation metadata columns to include in the dataset.\n",
    "        n_workers (int, optional): The number of worker processes for parallel computation. Defaults to 1 (sequential processing).\n",
    "\n",
    "    Methods:\n",
    "        generate_datasets(save: Optional[str] = None, merge: Optional[bool] = False): Converts `adata` into one or more Hugging Face datasets.\n",
    "        _prepare_features(): Prepares feature descriptions for the Hugging Face dataset based on `adata` structure.\n",
    "        _adata_to_dataset(adata_batch: AnnData, features: Features): Converts a single batch of AnnData to a Hugging Face Dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, adata: AnnData, columns: List[str] = ['total_counts', 'n_genes_by_counts'], batch_size: Optional[int] = None, gene_tokens: str = 'gene_token', n_workers: Optional[int] = 1):\n",
    "        \"\"\"\n",
    "        Initializes the AnnDataHFConverter_parallel object with the specified AnnData, batch size, gene tokens, columns, and number of workers.\n",
    "\n",
    "        Parameters:\n",
    "            adata (AnnData): The AnnData object to be processed.\n",
    "            columns (List[str]): Additional columns from `adata.obs` to be included in the datasets.\n",
    "            batch_size (int, optional): Number of observations to process per batch. If None, the whole dataset is processed as one batch.\n",
    "            gene_tokens (str): The key in the `adata.var` DataFrame that corresponds to the gene tokens.\n",
    "            n_workers (int, optional): Number of parallel worker processes to use. Defaults to 1 for sequential processing.\n",
    "        \"\"\"\n",
    "        self.adata = adata\n",
    "        self.batch_size = batch_size if batch_size is not None else self.adata.n_obs\n",
    "        self.gene_tokens = gene_tokens\n",
    "        self.columns = columns\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "    def generate_datasets(self, save: Optional[str] = None, merge: Optional[bool] = False) -> Optional[Union[Dataset, List[Dataset]]]:\n",
    "        \"\"\"\n",
    "        Generates and optionally saves and/or merges datasets from the AnnData object.\n",
    "\n",
    "        Parameters:\n",
    "            save (str, optional): Path to save individual batches as separate datasets. If not specified, datasets are not saved.\n",
    "            merge (bool, optional): Whether to merge all batch datasets into a single dataset. If True and `save` is specified, the merged dataset is saved.\n",
    "\n",
    "        Returns:\n",
    "            Union[Dataset, List[Dataset], None]: The resulting dataset(s) if not saved, or None if saved to disk.\n",
    "        \"\"\"\n",
    "        features = self._prepare_features()\n",
    "        total_cells = self.adata.n_obs\n",
    "\n",
    "        if save:\n",
    "            save_path = Path(save)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        datasets = []\n",
    "\n",
    "        def process_batch(idx, start_idx, end_idx):\n",
    "            adata_batch = self.adata[start_idx:end_idx]\n",
    "            dataset = self._adata_to_dataset(adata_batch, features)\n",
    "            if save:\n",
    "                batch_path = save_path / f\"{idx}.dataset\"\n",
    "                dataset.save_to_disk(str(batch_path))\n",
    "            del adata_batch\n",
    "            gc.collect()\n",
    "            return dataset\n",
    "\n",
    "        adata_splits = [(idx, start_idx, min(start_idx + self.batch_size, total_cells)) for idx, start_idx in enumerate(range(0, total_cells, self.batch_size))]\n",
    "        \n",
    "        if self.n_workers > 1:\n",
    "            datasets = Parallel(n_jobs=self.n_workers)(delayed(process_batch)(idx, start, end) for idx, start, end in adata_splits)\n",
    "        else:\n",
    "            for idx, start, end in adata_splits:\n",
    "                datasets.append(process_batch(idx, start, end))\n",
    "\n",
    "        if merge:\n",
    "            if save:\n",
    "                batches = list(save_path.iterdir())\n",
    "                batches_datasets = [load_from_disk(str(save_path / batch)) for batch in sorted(batches)]\n",
    "                dataset = concatenate_datasets(batches_datasets)\n",
    "                merged_path = save_path / \"merged.dataset\"\n",
    "                dataset.save_to_disk(str(merged_path))\n",
    "                return None\n",
    "            else:\n",
    "                dataset = concatenate_datasets(datasets)\n",
    "                return dataset\n",
    "        elif not save:\n",
    "            return datasets\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _prepare_features(self) -> Features:\n",
    "        \"\"\"\n",
    "        Prepares a Features object for the Hugging Face dataset based on the columns specified in the AnnData object.\n",
    "\n",
    "        Returns:\n",
    "            Features: A dictionary of features with data types corresponding to the columns in the AnnData object.\n",
    "        \"\"\"\n",
    "        features = {'gexp': Sequence(Value(\"float32\")), 'gene_token': Sequence(Value(\"int32\")), 'gene_name': Sequence(Value(\"string\"))}\n",
    "        if self.columns:\n",
    "            for col in self.columns:\n",
    "                if self.adata.obs[col].dtype.name in ['category', 'category_']:\n",
    "                    features[col] = Value(\"string\")\n",
    "                else:\n",
    "                    features[col] = Value(str(self.adata.obs[col].dtype))\n",
    "        return Features(features)\n",
    "\n",
    "    def _adata_to_dataset(self, adata_batch: AnnData, features: Features) -> Dataset:\n",
    "        \"\"\"\n",
    "        Converts a single batch of AnnData to a Hugging Face Dataset, more efficiently.\n",
    "\n",
    "        Parameters:\n",
    "            adata_batch (AnnData): A batch slice of the main AnnData object.\n",
    "            features (Features): The features for the dataset as prepared by `_prepare_features`.\n",
    "\n",
    "        Returns:\n",
    "            Dataset: A single batch converted into a Hugging Face Dataset.\n",
    "        \"\"\"\n",
    "        obs = adata_batch.obs\n",
    "        gexp = adata_batch.X.toarray() if not isinstance(adata_batch.X, np.ndarray) else adata_batch.X\n",
    "        gene_tokens = adata_batch.var[self.gene_tokens].to_numpy().astype(np.int32)\n",
    "        gene_names = adata_batch.var_names.to_numpy()\n",
    "        data_dict = {\n",
    "            'gexp': [gexp[cell] for cell in range(adata_batch.n_obs)],\n",
    "            'gene_token': [gene_tokens for _ in range(adata_batch.n_obs)],\n",
    "            'gene_name': [gene_names for _ in range(adata_batch.n_obs)]\n",
    "        }\n",
    "        if self.columns:\n",
    "            for col in self.columns:\n",
    "                data_dict[col] = adata_batch.obs[col].to_numpy()\n",
    "\n",
    "        dataset = Dataset.from_dict(data_dict, features=features)\n",
    "        dataset.set_format(type=\"torch\")\n",
    "\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe572578-180f-4f83-870e-6d7972bc2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/daniele/Code/scGraph/scgraph/resources/human_gene_vocab.json','r') as file:\n",
    "    h_d = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5a8de-4edb-4af7-9ffe-7f66bfe412b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = SetupAnnData(ad)\n",
    "ad_ = setup.process_adata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9215347-8518-42bb-99b9-323523c78647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "from typing import List, Optional\n",
    "\n",
    "class AnnDataTorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset to handle AnnData objects for processing with PyTorch models.\n",
    "    \n",
    "    Attributes:\n",
    "        adata (AnnData): An AnnData object containing the single-cell dataset.\n",
    "        gene_tokens (str): The column name in adata.var representing gene tokens.\n",
    "        columns (List[str]): List of additional observation metadata columns to include in the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, adata: AnnData, gene_tokens: str = 'gene_token', columns: List[str] = ['total_counts', 'n_genes_by_counts']):\n",
    "        \"\"\"\n",
    "        Initializes the AnnDataTorchDataset object with the specified AnnData and configuration.\n",
    "\n",
    "        Parameters:\n",
    "            adata (AnnData): The AnnData object to be processed.\n",
    "            gene_tokens (str): The key in the adata.var DataFrame that corresponds to the gene tokens.\n",
    "            columns (List[str]): Additional columns from adata.obs to be included in the datasets.\n",
    "        \"\"\"\n",
    "        self.adata = adata\n",
    "        self.gene_tokens = gene_tokens\n",
    "        self.columns = columns\n",
    "\n",
    "        self.gexp = self.adata.X.toarray() if not isinstance(self.adata.X, np.ndarray) else self.adata.X\n",
    "        self.gene_tokens = self.adata.var[gene_tokens].to_numpy().astype(np.int32)\n",
    "        self.gene_names = self.adata.var_names.to_numpy()\n",
    "        self.obs_data = {col: self.adata.obs[col].to_numpy() for col in self.columns}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.adata.n_obs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_data = {\n",
    "            'gexp': torch.tensor(self.gexp[idx], dtype=torch.float32),\n",
    "            'gene_token': torch.tensor(self.gene_tokens, dtype=torch.int32),\n",
    "            #'gene_name': self.gene_names  # This might need adjustment if you want it as a tensor\n",
    "        }\n",
    "        \n",
    "        for col in self.columns:\n",
    "            sample_data[col] = [self.adata.obs[col][idx]]#.to_numpy()\n",
    "\n",
    "        return sample_data\n",
    "\n",
    "# Example usage\n",
    "# Load your AnnData object somehow, for example:\n",
    "# adata = AnnData.read_h5ad('your_data_file.h5ad')\n",
    "# dataset = AnnDataTorchDataset(adata)\n",
    "# DataLoader can be used to create batches from the dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24293425-3e04-4b88-907d-22cb6420a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_human = pd.read_parquet('/home/daniele/Code/scGraph/scgraph/protein_embeddings/human_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c9d26-67ff-46d5-b71f-81c42263d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_mouse = pd.read_parquet('/home/daniele/Code/scGraph/scgraph/protein_embeddings/mouse_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd2b93-ebce-45c2-871f-5ecac9394003",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_genes = set(ad.var_names).intersection(set(esm_human.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d8cd1-5dcd-48a2-85c3-656ac7a71ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_genes = set(ad_m.var_names).intersection(set(esm_mouse.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2652eb8-6f01-4a2c-88a2-457255b0d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_dict = {'protein_coding_genes_human': list(human_genes), 'protein_coding_genes_mouse': list(mouse_genes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5c5f9-d6c5-480e-9e51-94561ae2a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = '/home/daniele/Code/scGraph/scgraph/resources/gene_dictionary.json'\n",
    "with open(json_file, 'w') as file:\n",
    "    json.dump(gene_dict, file, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbcab1a-dc24-45cc-a0f9-beb62bbb56ce",
   "metadata": {},
   "source": [
    "# Try esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4d77f-5583-4d74-8f6e-528eef756969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file = '/home/daniele/Code/scGraph/scgraph/resources/gene_dictionary.json'\n",
    "with open(json_file, 'r') as file:\n",
    "    gene_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40432a3-a0dc-4018-960a-72e9f91bf87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file = '/home/daniele/Code/scGraph/scgraph/resources/human_gene_vocab.json'\n",
    "with open(json_file, 'r') as file:\n",
    "    human_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "346260ad-0ab6-42a0-98e9-c68deb91751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file = '/home/daniele/Code/scGraph/scgraph/resources/mouse_gene_vocab.json'\n",
    "with open(json_file, 'r') as file:\n",
    "    m_d = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efb725-e776-4138-8cfb-a50c87f81864",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(h_d.keys()).intersection(set(ad.var_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e47b9-7839-4324-ac7e-cb4733b795b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(m_d.keys()).intersection(set(ad_m.var_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd3440-85e7-4b77-8cf4-38cdd5d38cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file = '/home/daniele/Code/scGraph/scgraph/resources/human_gene_vocab.json'\n",
    "with open(json_file, 'w') as file:\n",
    "    json.dump(human_dict,file,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742ae72-b919-4519-8ea9-27111520f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file = '/home/daniele/Code/scGraph/scgraph/resources/mouse_gene_vocab.json'\n",
    "with open(json_file, 'w') as file:\n",
    "    json.dump(mouse_dict,file,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2814161-d4be-42d4-b203-d009d271eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ortho = pd.read_table('/home/daniele/orthologs_table.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7f02332-68e0-429a-828d-17621ed673d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_entrez_gene</th>\n",
       "      <th>human_ensembl_gene</th>\n",
       "      <th>human_assert_ids</th>\n",
       "      <th>mouse_entrez_gene</th>\n",
       "      <th>mouse_ensembl_gene</th>\n",
       "      <th>mouse_assert_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>ENSG00000212395</td>\n",
       "      <td>ENSG00000212395</td>\n",
       "      <td>-</td>\n",
       "      <td>ENSMUSG00002076924</td>\n",
       "      <td>ENSMUSG00002076924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>ENSG00000212371</td>\n",
       "      <td>ENSG00000212371</td>\n",
       "      <td>115487044</td>\n",
       "      <td>ENSMUSG00000065089</td>\n",
       "      <td>ENSMUSG00000065089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>ENSG00000252473</td>\n",
       "      <td>ENSG00000252473</td>\n",
       "      <td>115489211</td>\n",
       "      <td>ENSMUSG00000119329</td>\n",
       "      <td>ENSMUSG00000119329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>ENSG00000252408</td>\n",
       "      <td>ENSG00000252408</td>\n",
       "      <td>115489209</td>\n",
       "      <td>ENSMUSG00000064536</td>\n",
       "      <td>ENSMUSG00000064536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>ENSG00000252762</td>\n",
       "      <td>ENSG00000252762</td>\n",
       "      <td>-</td>\n",
       "      <td>ENSMUSG00002076040</td>\n",
       "      <td>ENSMUSG00002076040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68075</th>\n",
       "      <td>79699</td>\n",
       "      <td>ENSG00000162378</td>\n",
       "      <td>OG6_107195,79699,ENOG5035HRW,ENSP00000294353,1...</td>\n",
       "      <td>414872</td>\n",
       "      <td>ENSMUSG00000034636</td>\n",
       "      <td>OG6_107195,414872,ENOG5035HRW,ENSMUSP000000438...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68076</th>\n",
       "      <td>79699</td>\n",
       "      <td>ENSG00000162378</td>\n",
       "      <td>OG6_107195</td>\n",
       "      <td>230590</td>\n",
       "      <td>ENSMUSG00000034645</td>\n",
       "      <td>OG6_107195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68077</th>\n",
       "      <td>7791</td>\n",
       "      <td>ENSG00000159840</td>\n",
       "      <td>621894at9347,OG6_127905,ENSP00000324422,HUMAN9...</td>\n",
       "      <td>22793</td>\n",
       "      <td>ENSMUSG00000029860</td>\n",
       "      <td>621894at9347,OG6_127905,ENSMUSP00000070427,MOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68078</th>\n",
       "      <td>23140</td>\n",
       "      <td>ENSG00000074755</td>\n",
       "      <td>9027,O43149,HUMAN|HGNC=29027|UniProtKB=O43149,...</td>\n",
       "      <td>195018</td>\n",
       "      <td>ENSMUSG00000055670</td>\n",
       "      <td>9027,Q5SSH7,MOUSE|MGI=MGI=2444286|UniProtKB=Q5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68079</th>\n",
       "      <td>26009</td>\n",
       "      <td>ENSG00000036549</td>\n",
       "      <td>9182,ENSG00000036549,HUMAN|Ensembl=ENSG0000003...</td>\n",
       "      <td>108946</td>\n",
       "      <td>ENSMUSG00000039068</td>\n",
       "      <td>9182,ENSMUSG00000039068,MOUSE|MGI=MGI=1920453|...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68080 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      human_entrez_gene human_ensembl_gene  \\\n",
       "0                     -    ENSG00000212395   \n",
       "1                     -    ENSG00000212371   \n",
       "2                     -    ENSG00000252473   \n",
       "3                     -    ENSG00000252408   \n",
       "4                     -    ENSG00000252762   \n",
       "...                 ...                ...   \n",
       "68075             79699    ENSG00000162378   \n",
       "68076             79699    ENSG00000162378   \n",
       "68077              7791    ENSG00000159840   \n",
       "68078             23140    ENSG00000074755   \n",
       "68079             26009    ENSG00000036549   \n",
       "\n",
       "                                        human_assert_ids mouse_entrez_gene  \\\n",
       "0                                        ENSG00000212395                 -   \n",
       "1                                        ENSG00000212371         115487044   \n",
       "2                                        ENSG00000252473         115489211   \n",
       "3                                        ENSG00000252408         115489209   \n",
       "4                                        ENSG00000252762                 -   \n",
       "...                                                  ...               ...   \n",
       "68075  OG6_107195,79699,ENOG5035HRW,ENSP00000294353,1...            414872   \n",
       "68076                                         OG6_107195            230590   \n",
       "68077  621894at9347,OG6_127905,ENSP00000324422,HUMAN9...             22793   \n",
       "68078  9027,O43149,HUMAN|HGNC=29027|UniProtKB=O43149,...            195018   \n",
       "68079  9182,ENSG00000036549,HUMAN|Ensembl=ENSG0000003...            108946   \n",
       "\n",
       "       mouse_ensembl_gene                                   mouse_assert_ids  \n",
       "0      ENSMUSG00002076924                                 ENSMUSG00002076924  \n",
       "1      ENSMUSG00000065089                                 ENSMUSG00000065089  \n",
       "2      ENSMUSG00000119329                                 ENSMUSG00000119329  \n",
       "3      ENSMUSG00000064536                                 ENSMUSG00000064536  \n",
       "4      ENSMUSG00002076040                                 ENSMUSG00002076040  \n",
       "...                   ...                                                ...  \n",
       "68075  ENSMUSG00000034636  OG6_107195,414872,ENOG5035HRW,ENSMUSP000000438...  \n",
       "68076  ENSMUSG00000034645                                         OG6_107195  \n",
       "68077  ENSMUSG00000029860  621894at9347,OG6_127905,ENSMUSP00000070427,MOU...  \n",
       "68078  ENSMUSG00000055670  9027,Q5SSH7,MOUSE|MGI=MGI=2444286|UniProtKB=Q5...  \n",
       "68079  ENSMUSG00000039068  9182,ENSMUSG00000039068,MOUSE|MGI=MGI=1920453|...  \n",
       "\n",
       "[68080 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ae3ddc-3002-440b-94de-ed9be465fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_d_ensembl = {v[1]:k for k,v in human_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c6cded-3768-4bbd-b62f-48848926337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_subset = ortho[ortho['mouse_ensembl_gene'].isin([v[1] for v in m_d.values()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e840941-7171-431d-9779-a821ab5be1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16076"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ortho_subset.mouse_ensembl_gene.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20e02f32-bcbd-4bba-9487-f552117db0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16161"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([v[1] for v in m_d.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dbc9791-4b2b-461d-8527-22f7852b93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {k:v for k,v in zip(ortho['mouse_ensembl_gene'],ortho['human_ensembl_gene'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf5f39-926f-4c38-b292-33561a68b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246f7d9-25eb-438b-9c8a-1aed972b1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for i in range(ortho_subset.shape[0]):\n",
    "    mapping["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25cb01-41ca-40f8-af2a-98417c197779",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for k in mapping.keys():\n",
    "    human = mapping[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2964b-351d-4b04-b389-568d95e31a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_last = {k:v for k,v in zip("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ed8ae-9a8f-4c2e-b05e-2aff74eb66d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116b14c-25f6-418f-a5a7-69c25a8c6388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d291c00b-ff4c-4a14-a9dc-521011ff1271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.isbacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f86ba1ae-6c6e-4467-a9d9-75e59803c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from anndata import AnnData\n",
    "from scipy.sparse import csr_matrix, hstack, issparse\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class AnnDataProcessor:\n",
    "    def __init__(self, adata: AnnData, protein_embeddings: Path, var_names: Optional[str] = None, isMouse: Optional[bool] = False) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the processor with an AnnData object, specifying options for gene identifiers and QC metrics.\n",
    "\n",
    "        Args:\n",
    "            adata (AnnData): The AnnData object to be processed.\n",
    "            protein_embeddings(Path): path to the esm protein embeddings stored as parquet file.\n",
    "            var_names (Optional[str]): Column in `adata.var` to use for gene identifiers, defaults to `adata.var_names` if None.\n",
    "            isMouse (Optional[bool]): Set to True if the dataset originates from mice, influencing the loaded gene vocabularies.\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(\"Initializing AnnDataProcessor\")\n",
    "        #self.genes_dictionary = self._load_genes_dictionary('mouse_gene_vocab.json' if isMouse else 'human_gene_vocab.json')\n",
    "        self.genes_dictionary = human_dict\n",
    "        self.protein_coding_genes = set(self.genes_dictionary.keys())\n",
    "        self.adata = adata\n",
    "        self.protein_embeddings = protein_embeddings\n",
    "        self.adata.var_names = self.adata.var[var_names] if var_names else self.adata.var_names\n",
    "        if 'total_counts' not in self.adata.obs.columns:\n",
    "            logging.info(\"total_counts not detected, computing QC metrics.\")\n",
    "            sc.pp.calculate_qc_metrics(self.adata, inplace = True)\n",
    "        logging.info(\"AnnDataProcessor initialized successfully\")\n",
    "\n",
    "    def process(self) -> Tuple[AnnData, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Processes the AnnData object by ensuring all required genes are present, and computing QC metrics if specified.\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(\"Processing AnnData object\")\n",
    "        missing_genes = self.protein_coding_genes - set(self.adata.var_names)\n",
    "        if missing_genes:\n",
    "            logging.info(f\"Extending AnnData with {len(missing_genes)} missing genes\")\n",
    "            self._extend_anndata(list(missing_genes))\n",
    "        else:\n",
    "            self.adata.var['extended'] = 0\n",
    "        self.adata = self.adata[:, self.adata.var_names.isin(self.protein_coding_genes)].copy()\n",
    "        self.adata.var['feature_id'] = [self.genes_dictionary[gene][1] for gene in self.adata.var_names]\n",
    "        self.adata.var['feature_name'] = self.adata.var_names\n",
    "        self.adata.var = self.adata.var[['feature_id', 'feature_name', 'extended']]        \n",
    "        logging.info(\"AnnData preprocessed succesfully.\")\n",
    "        logging.info(\"Reading esm protein embeddings\")\n",
    "        self.adata = self.adata[:,list(self.protein_coding_genes)]\n",
    "        protein_embeddings = pd.read_parquet(self.protein_embeddings).loc[list(self.protein_coding_genes)]        \n",
    "        return self.adata, protein_embeddings\n",
    "    \n",
    "    def _load_genes_dictionary(self, filename: str) -> Dict[str, List[Union[int, str]]]:\n",
    "        \"\"\"\n",
    "        Load a JSON file containing gene information and return it as a dictionary.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Filename of the JSON to load, relative to a predefined directory.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Union[int, str]]]: Dictionary containing gene information.\n",
    "        \"\"\"\n",
    "        \n",
    "        relative_path = Path(__file__).parent / 'resources' / filename\n",
    "        with open(relative_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _extend_anndata(self, missing_genes: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Extend the AnnData object by adding missing genes. Adds these genes to the var DataFrame and sets their expression values to -1 (masked).\n",
    "\n",
    "        Args:\n",
    "            missing_genes (List[str]): List of gene names that are missing and need to be added.\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(f\"Adding {len(missing_genes)} missing genes to AnnData\")\n",
    "        new_var = pd.DataFrame(index=missing_genes)\n",
    "        new_var['extended'] = 1  # Mark these genes as extended\n",
    "        if not issparse(self.adata.X):\n",
    "            self.adata.X = csr_matrix(self.adata.X)\n",
    "        masked_matrix = csr_matrix((self.adata.n_obs, len(missing_genes)), dtype=float)\n",
    "        new_X = hstack([self.adata.X, masked_matrix]) \n",
    "        extended_var = pd.concat([self.adata.var, new_var], axis=0)\n",
    "        extended_var['extended'] = extended_var['extended'].fillna(value = 0)\n",
    "\n",
    "        self.adata = AnnData(\n",
    "            X=new_X,\n",
    "            obs=self.adata.obs,\n",
    "            var=extended_var\n",
    "        )\n",
    "        logging.info(\"AnnData extension complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cea332d6-87d7-4cac-a1fe-f8c1b3cec74b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union, Generator\n",
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "from datasets import Dataset, Features, Sequence, Value, Array2D, concatenate_datasets, load_from_disk\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class AnnDataHFConverter:\n",
    "    \"\"\"\n",
    "    A class to convert AnnData objects to Hugging Face datasets with support for batching and parallel processing.\n",
    "\n",
    "    Attributes:\n",
    "        adata (AnnData): The AnnData object containing the single-cell dataset to be converted.\n",
    "        batch_size (Optional[int]): The size of each batch to process. Defaults to the number of observations in `adata`.\n",
    "        columns (List[str]): Additional observation metadata columns to include in the dataset.\n",
    "        n_workers (int): The number of worker processes for parallel computation. Defaults to 1 (sequential processing).\n",
    "        save (Optional[str]): Path to save the processed datasets.\n",
    "        merge (bool): Whether to merge all processed batches into a single dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, adata: AnnData, covariates: Optional[Union[str, List[str]]] = None, batch_size: Optional[int] = None, save: Optional[str] = None, merge: bool = True, n_workers: int = 1, cache_dir: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initializes the AnnDataHFConverter object with the specified AnnData, batch size, columns, and number of workers.\n",
    "\n",
    "        Parameters:\n",
    "            adata (AnnData): The AnnData object to convert, preprocessed with the `AnnDataProcessor` class.\n",
    "            columns (List[str]): Additional columns from `adata.obs` to be included in the datasets.\n",
    "            batch_size (Optional[int]): Number of observations to process per batch. If None, processes the entire dataset as one batch.\n",
    "            save (Optional[str]): Path to save individual batches as separate datasets. If not specified, datasets are not saved.\n",
    "            merge (bool): Whether to merge all batch datasets into a single dataset. If True and `save` is specified, the merged dataset is saved.\n",
    "            n_workers (int): Number of parallel worker processes to use. Defaults to 1 for sequential processing.\n",
    "            cache_dir(Optional[str]): the cache dir used by Hugging face to store the dataset while generating it.\n",
    "        \"\"\"\n",
    "\n",
    "        self.adata = adata\n",
    "        self.covariates = ['total_counts', 'n_genes_by_counts']#, 'extended']\n",
    "        self.cache_dir = cache_dir\n",
    "        #assert 'extended' in self.adata.var.columns, \"Please preprocess the AnnData with the `AnnDataProcessor` class.\"\n",
    "        \n",
    "        if covariates:\n",
    "            if isinstance(covariates, str):\n",
    "                self.covariates.append(covariates)\n",
    "            elif isinstance(covariates, list):\n",
    "                self.covariates.extend(covariates)\n",
    "            else:\n",
    "                raise ValueError(\"additional_covariates must be either a string or a list of strings.\")\n",
    "                \n",
    "        self.batch_size = batch_size if batch_size is not None else self.adata.n_obs\n",
    "        self.n_workers = n_workers\n",
    "        self.save = save\n",
    "        self.merge = merge\n",
    "        \n",
    "        if (self.merge or n_workers!=1) and self.batch_size == self.adata.n_obs:\n",
    "            self.merge = False\n",
    "            self.n_workers = 1\n",
    "       \n",
    "    def generate_datasets(self) -> Optional[Union[Dataset, List[Dataset]]]:\n",
    "        \"\"\"\n",
    "        Generates and optionally saves and/or merges datasets from the AnnData object.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Union[Dataset, List[Dataset]]]: The resulting dataset(s) if not saved, or None if saved to disk.\n",
    "        \"\"\"\n",
    "        \n",
    "        features = self._prepare_features()\n",
    "        \n",
    "        if self.save:\n",
    "            save_path = Path(self.save)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        datasets = []\n",
    "\n",
    "        def process_batch(idx, start_idx, end_idx):\n",
    "            adata_batch = self.adata[start_idx:end_idx]#.copy()\n",
    "            generator = self._create_generator(adata_batch)\n",
    "            dataset = Dataset.from_generator(generator=generator, features=features, cache_dir=self.cache_dir)\n",
    "            dataset.set_format(type=\"torch\")\n",
    "            batch_path = save_path / f\"batch_{idx}.dataset\" if self.save else None\n",
    "            if self.save:\n",
    "                dataset.save_to_disk(str(batch_path))\n",
    "            del adata_batch\n",
    "            gc.collect()\n",
    "            return dataset, batch_path\n",
    "\n",
    "        adata_splits = [(idx, start_idx, min(start_idx + self.batch_size, self.adata.n_obs)) for idx, start_idx in enumerate(range(0, self.adata.n_obs, self.batch_size))]\n",
    "        \n",
    "        if self.n_workers > 1:\n",
    "            results = Parallel(n_jobs=self.n_workers)(delayed(process_batch)(idx, start, end) for idx, start, end in adata_splits)\n",
    "        else:\n",
    "            results = [process_batch(idx, start, end) for idx, start, end in adata_splits]\n",
    "\n",
    "        datasets, paths = zip(*results) if results else ([], [])\n",
    "\n",
    "        if self.merge:\n",
    "            dataset = concatenate_datasets(list(datasets))\n",
    "            if self.save:\n",
    "                merged_path = save_path / \"merged.dataset\"\n",
    "                dataset.save_to_disk(str(merged_path))\n",
    "                for path in paths:\n",
    "                    if path and path.exists():\n",
    "                        shutil.rmtree(str(path))\n",
    "                print(f\"Individual batches removed, merged file at: {merged_path}\")\n",
    "            return dataset\n",
    "\n",
    "        return list(datasets) if not self.save else None\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _prepare_features(self) -> Features:\n",
    "        \"\"\"\n",
    "        Prepares a Features object for the Hugging Face dataset based on the columns specified in the AnnData object.\n",
    "    \n",
    "        Returns:\n",
    "            Features: A dictionary of features with data types corresponding to the columns in the AnnData object.\n",
    "        \"\"\"\n",
    "        \n",
    "        features = {\n",
    "            'gexp': Sequence(Value(\"float32\")),\n",
    "            'gene_names': Sequence(Value(\"string\"))\n",
    "        }\n",
    "        for col in self.covariates:\n",
    "            if col in self.adata.obs.columns :\n",
    "                dtype = self.adata.obs[col].dtype\n",
    "                features[col] = Value(\"string\") if dtype.name in ['category', 'object'] else Value(str(dtype))\n",
    "            elif col in self.adata.var.columns:\n",
    "                dtype = self.adata.var[col].dtype\n",
    "                features[col] = Sequence(Value(\"string\")) if dtype.name in ['category', 'object'] else Sequence(Value(str(dtype)))        \n",
    "            else:\n",
    "                raise ValueError(f'Covariate {col} not found in either .obs or .var')\n",
    "            \n",
    "        return Features(features)\n",
    "     \n",
    "    def _create_generator(self, adata_batch: AnnData) -> Generator:\n",
    "        \"\"\"\n",
    "        Create a generator function that can be passed to Dataset.from_generator.\n",
    "\n",
    "        Parameters:\n",
    "        adata_batch(\n",
    "        \"\"\"\n",
    "        def generator():\n",
    "            for i in range(adata_batch.n_obs):\n",
    "                cell_data = {\n",
    "                    'gexp': adata_batch.X.getrow(i).A.squeeze(),\n",
    "                    'gene_names': adata_batch.var_names.values\n",
    "                }\n",
    "                    \n",
    "                for col in self.covariates:\n",
    "                    if col in adata_batch.obs.columns:\n",
    "                        cell_data[col] = adata_batch.obs[col].iloc[i]\n",
    "                    elif col in adata_batch.var.columns:\n",
    "                        cell_data[col] = adata_batch.var[col].tolist()\n",
    "                    \n",
    "                yield cell_data\n",
    "        \n",
    "        return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6167ec5-ce03-4615-8f39-f245933a9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import anndata as ad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddd4fa9-ab3a-4abb-b7c8-2593ebf5d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnDataTorchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A Torch Dataset wrapper for an AnnData object to provide data cell by cell.\n",
    "\n",
    "    Attributes:\n",
    "        adata (ad.AnnData): The AnnData object containing the single-cell dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adata: ad.AnnData, protein_embeddings: pd.DataFrame, covariates: Optional[Union[str, List[str]]] = None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with an AnnData object.\n",
    "\n",
    "        Parameters:\n",
    "        adata (ad.AnnData): An AnnData object loaded with single-cell data.\n",
    "        \"\"\"\n",
    "        self.adata = adata\n",
    "        self.X = self.adata.X if isinstance(self.adata.X,np.ndarray) else self.adata.X.A\n",
    "        self.covariates = ['total_counts', 'n_genes_by_counts', 'extended']\n",
    "        self.protein_embeddings = protein_embeddings.values.squeeze()\n",
    "        assert 'extended' in self.adata.var.columns, \"Please preprocess the AnnData with the `AnnDataProcessor` class.\"\n",
    "        \n",
    "        if covariates:\n",
    "            if isinstance(covariates, str):\n",
    "                self.covariates.append(covariates)\n",
    "            elif isinstance(covariates, list):\n",
    "                self.covariates.extend(covariates)\n",
    "            else:\n",
    "                raise ValueError(\"additional_covariates must be either a string or a list of strings.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of cells in the dataset.\n",
    "        \"\"\"\n",
    "        return self.adata.n_obs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the expression values for the cell at the provided index.\n",
    "\n",
    "        Parameters:\n",
    "        idx (int): The index of the cell to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The expression values of the cell as a tensor.\n",
    "        \"\"\"\n",
    "        cell = {'gexp': self.X[idx], 'protein_embeddings': self.protein_embeddings}                 \n",
    "        for col in self.covariates:\n",
    "            if col in self.adata.obs.columns:\n",
    "                cell[col] = self.adata.obs[col].iloc[idx]\n",
    "            elif col in self.adata.var.columns:\n",
    "                cell[col] = torch.tensor(self.adata.var[col].values)\n",
    "\n",
    "                    \n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694190ca-5ffc-40da-849f-bfb866a37c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "            for col in ['total_counts', 'extended']:\n",
    "                if col in adata_batch.obs:\n",
    "                    cell_data[col] = adata_batch.obs[col].iloc[i]\n",
    "                elif col in adata_batch.var:\n",
    "                    cell_data[col] = adata_batch.var[col].values\n",
    "            \n",
    "            yield cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad7f528d-81d5-4439-a2d3-d5d16e787944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs Ã— n_vars = 673631 Ã— 60530 backed at '/mnt/storage/Daniele/preprocessed_data/cellxgene_census_human/0_preprocessed.h5ad'\n",
       "    obs: 'tissue_ontology_term_id', 'suspension_type', 'sex_ontology_term_id', 'cell_type', 'sex', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'disease', 'nnz', 'assay', 'development_stage_ontology_term_id', 'tissue', 'self_reported_ethnicity', 'development_stage', 'is_primary_data', 'soma_joinid', 'tissue_general_ontology_term_id', 'n_measured_vars', 'dataset_id', 'tissue_general', 'disease_ontology_term_id', 'tissue_type', 'raw_sum', 'raw_mean_nnz', 'self_reported_ethnicity_ontology_term_id', 'donor_id', 'raw_variance_nnz', 'observation_joinid', 'unique_donor_id', 'species', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'outlier'\n",
       "    var: 'feature_id', 'feature_name', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ef53687-e45c-4352-9410-f4877eac415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_cut = ad[:1000,:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59aad3b5-5396-4f3e-814c-a9339053aa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "View of AnnData object with n_obs Ã— n_vars = 1000 Ã— 1000 backed at '/mnt/storage/Daniele/preprocessed_data/cellxgene_census_human/0_preprocessed.h5ad'\n",
       "    obs: 'tissue_ontology_term_id', 'suspension_type', 'sex_ontology_term_id', 'cell_type', 'sex', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'disease', 'nnz', 'assay', 'development_stage_ontology_term_id', 'tissue', 'self_reported_ethnicity', 'development_stage', 'is_primary_data', 'soma_joinid', 'tissue_general_ontology_term_id', 'n_measured_vars', 'dataset_id', 'tissue_general', 'disease_ontology_term_id', 'tissue_type', 'raw_sum', 'raw_mean_nnz', 'self_reported_ethnicity_ontology_term_id', 'donor_id', 'raw_variance_nnz', 'observation_joinid', 'unique_donor_id', 'species', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes', 'total_counts_mt', 'log1p_total_counts_mt', 'pct_counts_mt', 'total_counts_ribo', 'log1p_total_counts_ribo', 'pct_counts_ribo', 'total_counts_hb', 'log1p_total_counts_hb', 'pct_counts_hb', 'outlier'\n",
       "    var: 'feature_id', 'feature_name', 'mt', 'ribo', 'hb', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1f9a1dd-7376-4183-bdb6-941b62581922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del ad_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92562a3c-4524-45ac-87bd-e3a00b073e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 11:03:52,110 - INFO - Initializing AnnDataProcessor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`. To load the object into memory, use `.to_memory()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ad_processor \u001b[38;5;241m=\u001b[39m \u001b[43mAnnDataProcessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mad_cut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/daniele/Code/scGraph/scgraph/protein_embeddings/human_embeddings.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 32\u001b[0m, in \u001b[0;36mAnnDataProcessor.__init__\u001b[0;34m(self, adata, protein_embeddings, var_names, isMouse)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata \u001b[38;5;241m=\u001b[39m adata\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprotein_embeddings \u001b[38;5;241m=\u001b[39m protein_embeddings\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_names\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata\u001b[38;5;241m.\u001b[39mvar[var_names] \u001b[38;5;28;01mif\u001b[39;00m var_names \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata\u001b[38;5;241m.\u001b[39mvar_names\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_counts\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata\u001b[38;5;241m.\u001b[39mobs\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     34\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_counts not detected, computing QC metrics.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/scanpy/lib/python3.9/site-packages/anndata/_core/anndata.py:957\u001b[0m, in \u001b[0;36mAnnData.var_names\u001b[0;34m(self, names)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;129m@var_names\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvar_names\u001b[39m(\u001b[38;5;28mself\u001b[39m, names: Sequence[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m    956\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_dim_index(names, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 957\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_dim_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/scanpy/lib/python3.9/site-packages/anndata/_core/anndata.py:907\u001b[0m, in \u001b[0;36mAnnData._set_dim_index\u001b[0;34m(self, value, attr)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_dim_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: pd\u001b[38;5;241m.\u001b[39mIndex, attr: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# Assumes _prep_dim_index has been run\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_view:\n\u001b[0;32m--> 907\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_as_actual(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/miniconda3/envs/scanpy/lib/python3.9/site-packages/anndata/_core/anndata.py:1600\u001b[0m, in \u001b[0;36mAnnData.copy\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_h5ad, write_h5ad\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1601\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo copy an AnnData object in backed mode, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a filename: `.copy(filename=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyfilename.h5ad\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo load the object into memory, use `.to_memory()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1604\u001b[0m     )\n\u001b[1;32m   1605\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39m_filemode\n\u001b[1;32m   1606\u001b[0m write_h5ad(filename, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: To copy an AnnData object in backed mode, pass a filename: `.copy(filename='myfilename.h5ad')`. To load the object into memory, use `.to_memory()`."
     ]
    }
   ],
   "source": [
    "ad_processor = AnnDataProcessor(ad_cut, '/home/daniele/Code/scGraph/scgraph/protein_embeddings/human_embeddings.parquet', var_names = 'feature_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cd53ff3-45fb-4f37-835d-ee0734891f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 00:04:56,634 - INFO - Processing AnnData object\n",
      "2024-05-06 00:04:56,643 - INFO - Extending AnnData with 18834 missing genes\n",
      "2024-05-06 00:04:56,646 - INFO - Adding 18834 missing genes to AnnData\n",
      "2024-05-06 00:04:56,698 - INFO - AnnData extension complete\n",
      "2024-05-06 00:04:57,353 - INFO - AnnData preprocessed succesfully.\n",
      "2024-05-06 00:04:57,354 - INFO - Reading esm protein embeddings\n"
     ]
    }
   ],
   "source": [
    "adata, embs = ad_processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aaebe29a-a5cd-4dac-8f20-681be5ff17f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_id</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>extended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OR4A15</th>\n",
       "      <td>ENSG00000181958</td>\n",
       "      <td>OR4A15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H2BC21</th>\n",
       "      <td>ENSG00000184678</td>\n",
       "      <td>H2BC21</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF235</th>\n",
       "      <td>ENSG00000159917</td>\n",
       "      <td>ZNF235</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCDHGA5</th>\n",
       "      <td>ENSG00000253485</td>\n",
       "      <td>PCDHGA5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AQP7B</th>\n",
       "      <td>ENSG00000259916</td>\n",
       "      <td>AQP7B</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRKCH</th>\n",
       "      <td>ENSG00000027075</td>\n",
       "      <td>PRKCH</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFIC</th>\n",
       "      <td>ENSG00000141905</td>\n",
       "      <td>NFIC</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSB</th>\n",
       "      <td>ENSG00000138385</td>\n",
       "      <td>SSB</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GFOD2</th>\n",
       "      <td>ENSG00000141098</td>\n",
       "      <td>GFOD2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRO</th>\n",
       "      <td>ENSG00000134042</td>\n",
       "      <td>MRO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19825 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature_id feature_name  extended\n",
       "OR4A15   ENSG00000181958       OR4A15       1.0\n",
       "H2BC21   ENSG00000184678       H2BC21       1.0\n",
       "ZNF235   ENSG00000159917       ZNF235       1.0\n",
       "PCDHGA5  ENSG00000253485      PCDHGA5       1.0\n",
       "AQP7B    ENSG00000259916        AQP7B       1.0\n",
       "...                  ...          ...       ...\n",
       "PRKCH    ENSG00000027075        PRKCH       0.0\n",
       "NFIC     ENSG00000141905         NFIC       1.0\n",
       "SSB      ENSG00000138385          SSB       1.0\n",
       "GFOD2    ENSG00000141098        GFOD2       1.0\n",
       "MRO      ENSG00000134042          MRO       1.0\n",
       "\n",
       "[19825 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52c791a4-72f8-43b7-b5a8-e2005e0d3f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2550</th>\n",
       "      <th>2551</th>\n",
       "      <th>2552</th>\n",
       "      <th>2553</th>\n",
       "      <th>2554</th>\n",
       "      <th>2555</th>\n",
       "      <th>2556</th>\n",
       "      <th>2557</th>\n",
       "      <th>2558</th>\n",
       "      <th>2559</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OR4A15</th>\n",
       "      <td>0.075709</td>\n",
       "      <td>-0.016977</td>\n",
       "      <td>0.116577</td>\n",
       "      <td>-0.047384</td>\n",
       "      <td>-0.055592</td>\n",
       "      <td>-0.097196</td>\n",
       "      <td>-0.125780</td>\n",
       "      <td>0.117031</td>\n",
       "      <td>0.041157</td>\n",
       "      <td>-0.107229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.070354</td>\n",
       "      <td>-0.046446</td>\n",
       "      <td>-0.039389</td>\n",
       "      <td>0.124881</td>\n",
       "      <td>0.014685</td>\n",
       "      <td>-0.161402</td>\n",
       "      <td>0.018862</td>\n",
       "      <td>-0.254390</td>\n",
       "      <td>-0.049054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H2BC21</th>\n",
       "      <td>0.024577</td>\n",
       "      <td>0.027296</td>\n",
       "      <td>-0.019227</td>\n",
       "      <td>-0.009875</td>\n",
       "      <td>0.129760</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>-0.078625</td>\n",
       "      <td>0.119971</td>\n",
       "      <td>0.117559</td>\n",
       "      <td>0.018290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014135</td>\n",
       "      <td>0.029447</td>\n",
       "      <td>0.070439</td>\n",
       "      <td>-0.011796</td>\n",
       "      <td>0.070686</td>\n",
       "      <td>-0.077867</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.050242</td>\n",
       "      <td>-0.089916</td>\n",
       "      <td>-0.002666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF235</th>\n",
       "      <td>0.017618</td>\n",
       "      <td>-0.014136</td>\n",
       "      <td>0.073148</td>\n",
       "      <td>-0.048830</td>\n",
       "      <td>-0.077148</td>\n",
       "      <td>-0.064344</td>\n",
       "      <td>-0.052398</td>\n",
       "      <td>0.126266</td>\n",
       "      <td>0.087634</td>\n",
       "      <td>0.047113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008405</td>\n",
       "      <td>0.042953</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.027062</td>\n",
       "      <td>0.033975</td>\n",
       "      <td>-0.056882</td>\n",
       "      <td>-0.025897</td>\n",
       "      <td>0.023816</td>\n",
       "      <td>-0.230112</td>\n",
       "      <td>0.015032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCDHGA5</th>\n",
       "      <td>-0.026696</td>\n",
       "      <td>-0.029550</td>\n",
       "      <td>-0.011700</td>\n",
       "      <td>-0.070574</td>\n",
       "      <td>-0.058281</td>\n",
       "      <td>-0.020026</td>\n",
       "      <td>-0.137816</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.035187</td>\n",
       "      <td>-0.072562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076399</td>\n",
       "      <td>-0.009979</td>\n",
       "      <td>-0.005953</td>\n",
       "      <td>0.008551</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>-0.061308</td>\n",
       "      <td>0.053673</td>\n",
       "      <td>-0.123048</td>\n",
       "      <td>-0.069099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AQP7B</th>\n",
       "      <td>-0.033831</td>\n",
       "      <td>-0.002237</td>\n",
       "      <td>0.149922</td>\n",
       "      <td>-0.070336</td>\n",
       "      <td>-0.074245</td>\n",
       "      <td>-0.082310</td>\n",
       "      <td>-0.046303</td>\n",
       "      <td>0.140770</td>\n",
       "      <td>0.112348</td>\n",
       "      <td>-0.066160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157253</td>\n",
       "      <td>-0.052053</td>\n",
       "      <td>0.021678</td>\n",
       "      <td>0.047585</td>\n",
       "      <td>0.156918</td>\n",
       "      <td>-0.076401</td>\n",
       "      <td>-0.015832</td>\n",
       "      <td>0.035389</td>\n",
       "      <td>-0.329794</td>\n",
       "      <td>-0.100048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRKCH</th>\n",
       "      <td>0.047953</td>\n",
       "      <td>-0.078331</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.063575</td>\n",
       "      <td>0.092070</td>\n",
       "      <td>-0.107469</td>\n",
       "      <td>-0.077264</td>\n",
       "      <td>0.058905</td>\n",
       "      <td>0.043936</td>\n",
       "      <td>-0.018209</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009887</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.055760</td>\n",
       "      <td>-0.072412</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.043553</td>\n",
       "      <td>-0.052560</td>\n",
       "      <td>-0.113003</td>\n",
       "      <td>-0.050934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFIC</th>\n",
       "      <td>0.029466</td>\n",
       "      <td>-0.002191</td>\n",
       "      <td>0.058328</td>\n",
       "      <td>-0.007980</td>\n",
       "      <td>-0.016677</td>\n",
       "      <td>-0.008492</td>\n",
       "      <td>-0.030325</td>\n",
       "      <td>0.076611</td>\n",
       "      <td>-0.025818</td>\n",
       "      <td>-0.044419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>-0.106382</td>\n",
       "      <td>0.027533</td>\n",
       "      <td>0.075052</td>\n",
       "      <td>0.039753</td>\n",
       "      <td>-0.021913</td>\n",
       "      <td>-0.018525</td>\n",
       "      <td>0.043329</td>\n",
       "      <td>-0.079458</td>\n",
       "      <td>0.008996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSB</th>\n",
       "      <td>0.011891</td>\n",
       "      <td>-0.036905</td>\n",
       "      <td>0.068846</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>-0.013581</td>\n",
       "      <td>-0.059306</td>\n",
       "      <td>-0.073502</td>\n",
       "      <td>0.078924</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>-0.008898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091435</td>\n",
       "      <td>-0.050210</td>\n",
       "      <td>0.063731</td>\n",
       "      <td>-0.097927</td>\n",
       "      <td>0.174676</td>\n",
       "      <td>-0.019236</td>\n",
       "      <td>0.048399</td>\n",
       "      <td>0.123177</td>\n",
       "      <td>-0.116802</td>\n",
       "      <td>-0.065504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GFOD2</th>\n",
       "      <td>0.006096</td>\n",
       "      <td>-0.028234</td>\n",
       "      <td>0.024738</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>-0.043456</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019945</td>\n",
       "      <td>-0.003638</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>-0.058379</td>\n",
       "      <td>-0.010992</td>\n",
       "      <td>-0.053960</td>\n",
       "      <td>-0.052604</td>\n",
       "      <td>0.069502</td>\n",
       "      <td>-0.070411</td>\n",
       "      <td>0.004945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRO</th>\n",
       "      <td>0.023791</td>\n",
       "      <td>-0.048914</td>\n",
       "      <td>0.059082</td>\n",
       "      <td>-0.023807</td>\n",
       "      <td>0.046623</td>\n",
       "      <td>-0.065950</td>\n",
       "      <td>-0.034534</td>\n",
       "      <td>0.172534</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>0.016569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135842</td>\n",
       "      <td>-0.026472</td>\n",
       "      <td>0.059877</td>\n",
       "      <td>-0.184270</td>\n",
       "      <td>0.149056</td>\n",
       "      <td>-0.107802</td>\n",
       "      <td>-0.070012</td>\n",
       "      <td>0.039518</td>\n",
       "      <td>-0.133772</td>\n",
       "      <td>-0.084435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19825 rows Ã— 2560 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6     \\\n",
       "OR4A15   0.075709 -0.016977  0.116577 -0.047384 -0.055592 -0.097196 -0.125780   \n",
       "H2BC21   0.024577  0.027296 -0.019227 -0.009875  0.129760  0.018530 -0.078625   \n",
       "ZNF235   0.017618 -0.014136  0.073148 -0.048830 -0.077148 -0.064344 -0.052398   \n",
       "PCDHGA5 -0.026696 -0.029550 -0.011700 -0.070574 -0.058281 -0.020026 -0.137816   \n",
       "AQP7B   -0.033831 -0.002237  0.149922 -0.070336 -0.074245 -0.082310 -0.046303   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "PRKCH    0.047953 -0.078331 -0.001375 -0.063575  0.092070 -0.107469 -0.077264   \n",
       "NFIC     0.029466 -0.002191  0.058328 -0.007980 -0.016677 -0.008492 -0.030325   \n",
       "SSB      0.011891 -0.036905  0.068846  0.002096 -0.013581 -0.059306 -0.073502   \n",
       "GFOD2    0.006096 -0.028234  0.024738 -0.004632  0.008666  0.013890 -0.043456   \n",
       "MRO      0.023791 -0.048914  0.059082 -0.023807  0.046623 -0.065950 -0.034534   \n",
       "\n",
       "             7         8         9     ...      2550      2551      2552  \\\n",
       "OR4A15   0.117031  0.041157 -0.107229  ...  0.199900  0.070354 -0.046446   \n",
       "H2BC21   0.119971  0.117559  0.018290  ... -0.014135  0.029447  0.070439   \n",
       "ZNF235   0.126266  0.087634  0.047113  ... -0.008405  0.042953  0.001227   \n",
       "PCDHGA5  0.121500  0.035187 -0.072562  ...  0.076399 -0.009979 -0.005953   \n",
       "AQP7B    0.140770  0.112348 -0.066160  ...  0.157253 -0.052053  0.021678   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "PRKCH    0.058905  0.043936 -0.018209  ... -0.009887  0.005473  0.007390   \n",
       "NFIC     0.076611 -0.025818 -0.044419  ...  0.036781 -0.106382  0.027533   \n",
       "SSB      0.078924  0.036888 -0.008898  ...  0.091435 -0.050210  0.063731   \n",
       "GFOD2    0.036703 -0.009366  0.000409  ... -0.019945 -0.003638  0.011175   \n",
       "MRO      0.172534 -0.009752  0.016569  ...  0.135842 -0.026472  0.059877   \n",
       "\n",
       "             2553      2554      2555      2556      2557      2558      2559  \n",
       "OR4A15  -0.039389  0.124881  0.014685 -0.161402  0.018862 -0.254390 -0.049054  \n",
       "H2BC21  -0.011796  0.070686 -0.077867  0.032035  0.050242 -0.089916 -0.002666  \n",
       "ZNF235   0.027062  0.033975 -0.056882 -0.025897  0.023816 -0.230112  0.015032  \n",
       "PCDHGA5  0.008551  0.006016 -0.002289 -0.061308  0.053673 -0.123048 -0.069099  \n",
       "AQP7B    0.047585  0.156918 -0.076401 -0.015832  0.035389 -0.329794 -0.100048  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "PRKCH    0.055760 -0.072412 -0.073535  0.043553 -0.052560 -0.113003 -0.050934  \n",
       "NFIC     0.075052  0.039753 -0.021913 -0.018525  0.043329 -0.079458  0.008996  \n",
       "SSB     -0.097927  0.174676 -0.019236  0.048399  0.123177 -0.116802 -0.065504  \n",
       "GFOD2   -0.058379 -0.010992 -0.053960 -0.052604  0.069502 -0.070411  0.004945  \n",
       "MRO     -0.184270  0.149056 -0.107802 -0.070012  0.039518 -0.133772 -0.084435  \n",
       "\n",
       "[19825 rows x 2560 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd15b0-0d3d-4677-bd52-a893a870a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51e40ae7-462b-46bd-be80-b1508f6c118a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Currently, you cannot index repeatedly into a backed AnnData, that is, you cannot make a view of a view.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m now \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m cv \u001b[38;5;241m=\u001b[39m AnnDataHFConverter(ad_cut, )\u001b[38;5;66;03m#batch_size = 100, n_workers = 10, merge = True)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m now)\n",
      "Cell \u001b[0;32mIn[43], line 91\u001b[0m, in \u001b[0;36mAnnDataHFConverter.generate_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_workers)(delayed(process_batch)(idx, start, end) \u001b[38;5;28;01mfor\u001b[39;00m idx, start, end \u001b[38;5;129;01min\u001b[39;00m adata_splits)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     results \u001b[38;5;241m=\u001b[39m [process_batch(idx, start, end) \u001b[38;5;28;01mfor\u001b[39;00m idx, start, end \u001b[38;5;129;01min\u001b[39;00m adata_splits]\n\u001b[1;32m     93\u001b[0m datasets, paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults) \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;28;01melse\u001b[39;00m ([], [])\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge:\n",
      "Cell \u001b[0;32mIn[43], line 91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     89\u001b[0m     results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_workers)(delayed(process_batch)(idx, start, end) \u001b[38;5;28;01mfor\u001b[39;00m idx, start, end \u001b[38;5;129;01min\u001b[39;00m adata_splits)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx, start, end \u001b[38;5;129;01min\u001b[39;00m adata_splits]\n\u001b[1;32m     93\u001b[0m datasets, paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults) \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;28;01melse\u001b[39;00m ([], [])\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge:\n",
      "Cell \u001b[0;32mIn[43], line 75\u001b[0m, in \u001b[0;36mAnnDataHFConverter.generate_datasets.<locals>.process_batch\u001b[0;34m(idx, start_idx, end_idx)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_batch\u001b[39m(idx, start_idx, end_idx):\n\u001b[0;32m---> 75\u001b[0m     adata_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;66;03m#.copy()\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generator(adata_batch)\n\u001b[1;32m     77\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_generator(generator\u001b[38;5;241m=\u001b[39mgenerator, features\u001b[38;5;241m=\u001b[39mfeatures, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/scanpy/lib/python3.9/site-packages/anndata/_core/anndata.py:1178\u001b[0m, in \u001b[0;36mAnnData.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a sliced view of the object.\"\"\"\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m oidx, vidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_indices(index)\n\u001b[0;32m-> 1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAnnData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moidx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvidx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/scanpy/lib/python3.9/site-packages/anndata/_core/anndata.py:360\u001b[0m, in \u001b[0;36mAnnData.__init__\u001b[0;34m(self, X, obs, var, uns, obsm, varm, layers, raw, dtype, shape, filename, filemode, asview, obsp, varp, oidx, vidx)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, AnnData):\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`X` has to be an AnnData object.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_as_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvidx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_as_actual(\n\u001b[1;32m    363\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    364\u001b[0m         obs\u001b[38;5;241m=\u001b[39mobs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         filemode\u001b[38;5;241m=\u001b[39mfilemode,\n\u001b[1;32m    377\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/scanpy/lib/python3.9/site-packages/anndata/_core/anndata.py:381\u001b[0m, in \u001b[0;36mAnnData._init_as_view\u001b[0;34m(self, adata_ref, oidx, vidx)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_as_view\u001b[39m(\u001b[38;5;28mself\u001b[39m, adata_ref: AnnData, oidx: Index, vidx: Index):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adata_ref\u001b[38;5;241m.\u001b[39misbacked \u001b[38;5;129;01mand\u001b[39;00m adata_ref\u001b[38;5;241m.\u001b[39mis_view:\n\u001b[0;32m--> 381\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, you cannot index repeatedly into a backed AnnData, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat is, you cannot make a view of a view.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    384\u001b[0m         )\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(oidx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger)):\n",
      "\u001b[0;31mValueError\u001b[0m: Currently, you cannot index repeatedly into a backed AnnData, that is, you cannot make a view of a view."
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "cv = AnnDataHFConverter(ad_cut, )#batch_size = 100, n_workers = 10, merge = True)\n",
    "dataset = cv.generate_datasets()\n",
    "print(time.time() - now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cfc1476-aa31-4aa8-8694-eeaf06d9f1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OR4A15',\n",
       " 'H2BC21',\n",
       " 'ZNF235',\n",
       " 'PCDHGA5',\n",
       " 'AQP7B',\n",
       " 'ETNK2',\n",
       " 'KALRN',\n",
       " 'PDZD11',\n",
       " 'PPP6R1',\n",
       " 'ZC3H7B',\n",
       " 'FAM120A',\n",
       " 'ATXN3L',\n",
       " 'SLC24A2',\n",
       " 'PAQR7',\n",
       " 'GCNT7',\n",
       " 'STK32C',\n",
       " 'MRPL36',\n",
       " 'ITGB1',\n",
       " 'RFTN2',\n",
       " 'BNIP5',\n",
       " 'FLOT2',\n",
       " 'CTXND1',\n",
       " 'PTPN13',\n",
       " 'SLC35A3',\n",
       " 'DEPDC1',\n",
       " 'HAUS5',\n",
       " 'AVP',\n",
       " 'ADPRM',\n",
       " 'VWDE',\n",
       " 'DGAT2',\n",
       " 'CBX2',\n",
       " 'ZNF398',\n",
       " 'IRX5',\n",
       " 'UGT2A3',\n",
       " 'USP14',\n",
       " 'WDR90',\n",
       " 'TRPC4AP',\n",
       " 'RCBTB2',\n",
       " 'CNTN1',\n",
       " 'ARTN',\n",
       " 'OR10A7',\n",
       " 'EMC6',\n",
       " 'KIAA0513',\n",
       " 'ARNT2',\n",
       " 'C6orf120',\n",
       " 'NFXL1',\n",
       " 'GSTP1',\n",
       " 'SLC37A2',\n",
       " 'CASC2',\n",
       " 'PODNL1',\n",
       " 'OR52J3',\n",
       " 'IGFN1',\n",
       " 'KRTAP10-4',\n",
       " 'METAP2',\n",
       " 'OSBPL2',\n",
       " 'TCEAL9',\n",
       " 'NNMT',\n",
       " 'CCDC141',\n",
       " 'IPMK',\n",
       " 'UGT1A9',\n",
       " 'CMKLR2',\n",
       " 'HACD2',\n",
       " 'PABPC1L',\n",
       " 'JDP2',\n",
       " 'PCYT1A',\n",
       " 'TEX264',\n",
       " 'TUSC3',\n",
       " 'TMED1',\n",
       " 'RER1',\n",
       " 'LCN9',\n",
       " 'HLCS',\n",
       " 'CCDC153',\n",
       " 'TAP2',\n",
       " 'ESR2',\n",
       " 'ST7',\n",
       " 'SPTY2D1',\n",
       " 'BTBD8',\n",
       " 'LMAN1',\n",
       " 'PDYN',\n",
       " 'FRY',\n",
       " 'WDR41',\n",
       " 'S1PR1',\n",
       " 'GOLGA8J',\n",
       " 'BET1L',\n",
       " 'IKZF4',\n",
       " 'CXCL9',\n",
       " 'TAMALIN',\n",
       " 'MPZL2',\n",
       " 'ACTL7A',\n",
       " 'OS9',\n",
       " 'SLCO1B3',\n",
       " 'SGO2',\n",
       " 'HS3ST6',\n",
       " 'CCNA1',\n",
       " 'PITPNM2',\n",
       " 'U2SURP',\n",
       " 'GDF5',\n",
       " 'CMTM7',\n",
       " 'GPR4',\n",
       " 'ADGRB2',\n",
       " 'POLR2M',\n",
       " 'ZNF117',\n",
       " 'RPL37AP8',\n",
       " 'DBNL',\n",
       " 'RNF169',\n",
       " 'PSMD14',\n",
       " 'GNG4',\n",
       " 'NOL4',\n",
       " 'NQO2',\n",
       " 'LIPA',\n",
       " 'GNAI1',\n",
       " 'DGLUCY',\n",
       " 'ROM1',\n",
       " 'TCF3',\n",
       " 'PAN3',\n",
       " 'JMJD8',\n",
       " 'TMED7',\n",
       " 'C9orf50',\n",
       " 'FANCD2',\n",
       " 'C10orf95',\n",
       " 'KRBOX4',\n",
       " 'SLC37A1',\n",
       " 'DGUOK',\n",
       " 'DEPDC5',\n",
       " 'TBCA',\n",
       " 'TLE4',\n",
       " 'NHLRC4',\n",
       " 'PLEKHH3',\n",
       " 'IMPG1',\n",
       " 'HAVCR1',\n",
       " 'MTHFD1L',\n",
       " 'UQCC1',\n",
       " 'NAP1L3',\n",
       " 'GIMAP4',\n",
       " 'INSRR',\n",
       " 'LAMP1',\n",
       " 'NDUFAB1',\n",
       " 'PLA2G4E',\n",
       " 'AOC2',\n",
       " 'NXPH4',\n",
       " 'OR13C9',\n",
       " 'MCFD2',\n",
       " 'TRPV1',\n",
       " 'SYT14',\n",
       " 'DYTN',\n",
       " 'OR11H4',\n",
       " 'PRM1',\n",
       " 'GIMD1',\n",
       " 'NDUFB10',\n",
       " 'RND1',\n",
       " 'THBS3',\n",
       " 'DNAJB7',\n",
       " 'TRAV6',\n",
       " 'PBX4',\n",
       " 'ZNF358',\n",
       " 'CAPN7',\n",
       " 'SRD5A2',\n",
       " 'COL5A3',\n",
       " 'WDR49',\n",
       " 'HNRNPA1L2',\n",
       " 'FKBP7',\n",
       " 'CCDC71L',\n",
       " 'NBPF4',\n",
       " 'COL9A3',\n",
       " 'MAGI3',\n",
       " 'TXLNB',\n",
       " 'H2AC21',\n",
       " 'CD7',\n",
       " 'TRIM49',\n",
       " 'C11orf42',\n",
       " 'ACER3',\n",
       " 'PRR22',\n",
       " 'PNMA1',\n",
       " 'NUP37',\n",
       " 'MUC2',\n",
       " 'CHCHD2P9',\n",
       " 'ANKRD33B',\n",
       " 'ZNF146',\n",
       " 'EIF2S2',\n",
       " 'NDUFA8',\n",
       " 'CYP27C1',\n",
       " 'NTNG1',\n",
       " 'NKAPD1',\n",
       " 'INCA1',\n",
       " 'NUDT3',\n",
       " 'RPS13',\n",
       " 'SULT1A2',\n",
       " 'CRKL',\n",
       " 'SPNS2',\n",
       " 'ADCY8',\n",
       " 'POU4F1',\n",
       " 'DPP6',\n",
       " 'RAI1',\n",
       " 'YRDC',\n",
       " 'PTAFR',\n",
       " 'MICAL2',\n",
       " 'RAMAC',\n",
       " 'GKN1',\n",
       " 'SPRYD7',\n",
       " 'ICAM3',\n",
       " 'ATAD3C',\n",
       " 'COQ10B',\n",
       " 'OR52K1',\n",
       " 'SFXN2',\n",
       " 'ACOT7',\n",
       " 'DNAH9',\n",
       " 'NEO1',\n",
       " 'LPAR1',\n",
       " 'IGHV3-21',\n",
       " 'SZT2',\n",
       " 'PTPN1',\n",
       " 'EBF2',\n",
       " 'SLC25A24',\n",
       " 'RHOD',\n",
       " 'SERPINA10',\n",
       " 'AKR1C1',\n",
       " 'ST20-AS1',\n",
       " 'KAZN',\n",
       " 'CT45A5',\n",
       " 'FOXD4L3',\n",
       " 'SRPX2',\n",
       " 'WASH4P',\n",
       " 'OXNAD1',\n",
       " 'VPS16',\n",
       " 'C21orf58',\n",
       " 'CFAP58',\n",
       " 'PTCH1',\n",
       " 'CAPNS1',\n",
       " 'TBC1D8B',\n",
       " 'MVB12B',\n",
       " 'RUFY3',\n",
       " 'CBR4',\n",
       " 'AKR7L',\n",
       " 'USP48',\n",
       " 'HDHD3',\n",
       " 'GALM',\n",
       " 'OR4N5',\n",
       " 'PFDN6',\n",
       " 'KIF11',\n",
       " 'ZNF764',\n",
       " 'SLC22A9',\n",
       " 'PRR12',\n",
       " 'CT45A8',\n",
       " 'KAT2B',\n",
       " 'RBMS1',\n",
       " 'RTTN',\n",
       " 'CWC15',\n",
       " 'FAM153A',\n",
       " 'VDAC1',\n",
       " 'ZNF532',\n",
       " 'SLC22A10',\n",
       " 'PRKCG',\n",
       " 'NAA40',\n",
       " 'XPOT',\n",
       " 'RANGAP1',\n",
       " 'WDR13',\n",
       " 'RP1',\n",
       " 'RFX2',\n",
       " 'TIAM2',\n",
       " 'ABHD4',\n",
       " 'MEGF8',\n",
       " 'MIGA1',\n",
       " 'ZC4H2',\n",
       " 'AP5S1',\n",
       " 'COL6A1',\n",
       " 'DRD1',\n",
       " 'RGS2',\n",
       " 'PADI4',\n",
       " 'ZHX1-C8orf76',\n",
       " 'MUC6',\n",
       " 'OAS1',\n",
       " 'EDIL3',\n",
       " 'ZNF593',\n",
       " 'PAPSS2',\n",
       " 'POLG2',\n",
       " 'CHP2',\n",
       " 'IFI27L1',\n",
       " 'PRODH2',\n",
       " 'CCDC91',\n",
       " 'CYP26A1',\n",
       " 'LEKR1',\n",
       " 'RADIL',\n",
       " 'TTBK2',\n",
       " 'HSD17B1',\n",
       " 'TOX3',\n",
       " 'HOXB2',\n",
       " 'COMMD4',\n",
       " 'ETDC',\n",
       " 'H2AC1',\n",
       " 'DBR1',\n",
       " 'SEC11B',\n",
       " 'PCSK7',\n",
       " 'BASP1',\n",
       " 'BEX3',\n",
       " 'RILPL2',\n",
       " 'IER3',\n",
       " 'EIF6',\n",
       " 'SEMA6C',\n",
       " 'TEX29',\n",
       " 'SPAG9',\n",
       " 'APBB3',\n",
       " 'KRTAP25-1',\n",
       " 'KCNJ5',\n",
       " 'GJA8',\n",
       " 'MRPS5',\n",
       " 'PIGBOS1',\n",
       " 'SEPTIN4',\n",
       " 'OR6C6',\n",
       " 'PKDCC',\n",
       " 'ZNF155',\n",
       " 'AKR1D1',\n",
       " 'MAF',\n",
       " 'ZNF304',\n",
       " 'TRPT1',\n",
       " 'COP1',\n",
       " 'WHRN',\n",
       " 'RBL2',\n",
       " 'LTA',\n",
       " 'SCYGR1',\n",
       " 'DDTL',\n",
       " 'TRAF3',\n",
       " 'KBTBD6',\n",
       " 'GLUL',\n",
       " 'ADGRF3',\n",
       " 'SERINC1',\n",
       " 'KCNIP3',\n",
       " 'HERPUD2',\n",
       " 'TRIM14',\n",
       " 'OR8G5',\n",
       " 'RBM6',\n",
       " 'SMIM36',\n",
       " 'TERT',\n",
       " 'GALNT12',\n",
       " 'TACO1',\n",
       " 'HELB',\n",
       " 'EOLA1',\n",
       " 'PCDH20',\n",
       " 'ADGRE5',\n",
       " 'CNTNAP2',\n",
       " 'RPS20',\n",
       " 'PGAM1',\n",
       " 'IAH1',\n",
       " 'MYL2',\n",
       " 'AHSG',\n",
       " 'PCTP',\n",
       " 'ACOT11',\n",
       " 'DDX4',\n",
       " 'CLEC6A',\n",
       " 'SPC25',\n",
       " 'MTMR4',\n",
       " 'NUDT16L1',\n",
       " 'TTC14',\n",
       " 'PRKCD',\n",
       " 'POC5',\n",
       " 'KHDC1',\n",
       " 'KNOP1',\n",
       " 'KRTAP24-1',\n",
       " 'GGTLC3',\n",
       " 'APOL5',\n",
       " 'GLI1',\n",
       " 'LY9',\n",
       " 'GALK1',\n",
       " 'EPGN',\n",
       " 'FSBP',\n",
       " 'RALA',\n",
       " 'SP8',\n",
       " 'HRG',\n",
       " 'C10orf105',\n",
       " 'PGLYRP1',\n",
       " 'E2F8',\n",
       " 'PSG4',\n",
       " 'DCC',\n",
       " 'GNG14',\n",
       " 'PCDHA8',\n",
       " 'ZNF705D',\n",
       " 'GLUD2',\n",
       " 'CC2D1A',\n",
       " 'GYG1',\n",
       " 'ZBTB3',\n",
       " 'OR4K13',\n",
       " 'DYNLL2',\n",
       " 'TIMM8B',\n",
       " 'OR51S1',\n",
       " 'GALE',\n",
       " 'SKIL',\n",
       " 'TRAV21',\n",
       " 'KRTAP19-5',\n",
       " 'ATOSB',\n",
       " 'VOPP1',\n",
       " 'MAN1C1',\n",
       " 'PRKD2',\n",
       " 'PPP2R5E',\n",
       " 'OR52N5',\n",
       " 'DDX56',\n",
       " 'CTSC',\n",
       " 'SLC35E4',\n",
       " 'PLK1',\n",
       " 'DCUN1D4',\n",
       " 'MAGEB6',\n",
       " 'ADCK2',\n",
       " 'GAR1',\n",
       " 'ZNF439',\n",
       " 'STMND1',\n",
       " 'ARMC5',\n",
       " 'TMEM179',\n",
       " 'ZSWIM5',\n",
       " 'MAP3K12',\n",
       " 'HPS3',\n",
       " 'SLC18A3',\n",
       " 'USP49',\n",
       " 'NR4A3',\n",
       " 'ZDHHC11B',\n",
       " 'CLDN24',\n",
       " 'CLEC12B',\n",
       " 'MORN5',\n",
       " 'CGB2',\n",
       " 'HECA',\n",
       " 'DUT',\n",
       " 'CBLL1',\n",
       " 'REG4',\n",
       " 'DYNLT3',\n",
       " 'FAM177A1',\n",
       " 'INPP5K',\n",
       " 'GEMIN8',\n",
       " 'LPIN3',\n",
       " 'CAMK1',\n",
       " 'GPR146',\n",
       " 'CHPF2',\n",
       " 'ZNF75D',\n",
       " 'MARK2',\n",
       " 'SP3',\n",
       " 'CBLC',\n",
       " 'IGKV1-33',\n",
       " 'INTS1',\n",
       " 'EHD4',\n",
       " 'TRIM25',\n",
       " 'ELP4',\n",
       " 'RABGGTB',\n",
       " 'OOSP2',\n",
       " 'GAS8-AS1',\n",
       " 'RGS9',\n",
       " 'LHB',\n",
       " 'DHX35',\n",
       " 'PUSL1',\n",
       " 'CBX1',\n",
       " 'AICDA',\n",
       " 'FAM9B',\n",
       " 'PROP1',\n",
       " 'HSD17B2',\n",
       " 'ZBED3',\n",
       " 'AMBP',\n",
       " 'RTCA',\n",
       " 'CLDN9',\n",
       " 'CHADL',\n",
       " 'LRRC2',\n",
       " 'LUM',\n",
       " 'ORC4',\n",
       " 'OR8K1',\n",
       " 'ARPP21',\n",
       " 'TEX46',\n",
       " 'RIC8A',\n",
       " 'REC8',\n",
       " 'POLR2J2',\n",
       " 'TFF1',\n",
       " 'IHO1',\n",
       " 'PODN',\n",
       " 'BEST4',\n",
       " 'PPFIA3',\n",
       " 'ATP6V1F',\n",
       " 'ARGLU1',\n",
       " 'AGBL5',\n",
       " 'DCSTAMP',\n",
       " 'TMSB4Y',\n",
       " 'SETDB2',\n",
       " 'FAM186A',\n",
       " 'ARHGAP19',\n",
       " 'FZD10',\n",
       " 'TEX22',\n",
       " 'CDH13',\n",
       " 'ACSM4',\n",
       " 'HEBP2',\n",
       " 'PLXND1',\n",
       " 'TSC22D3',\n",
       " 'CFAP410',\n",
       " 'VNN1',\n",
       " 'ABCD4',\n",
       " 'C11orf16',\n",
       " 'MATCAP1',\n",
       " 'SERPINB4',\n",
       " 'TRPC6',\n",
       " 'OR8D1',\n",
       " 'LASP1',\n",
       " 'GPSM3',\n",
       " 'MRPL27',\n",
       " 'ICA1L',\n",
       " 'ACTL10',\n",
       " 'AMOTL1',\n",
       " 'NMRAL1',\n",
       " 'TMEM105',\n",
       " 'VPS41',\n",
       " 'PSPN',\n",
       " 'HOGA1',\n",
       " 'MFF',\n",
       " 'PEX11B',\n",
       " 'TNC',\n",
       " 'TSEN15',\n",
       " 'IGLV3-22',\n",
       " 'NDN',\n",
       " 'TMEM74B',\n",
       " 'TRAFD1',\n",
       " 'CHERP',\n",
       " 'CCL24',\n",
       " 'YARS1',\n",
       " 'PRR23B',\n",
       " 'AFMID',\n",
       " 'IL34',\n",
       " 'OR5AP2',\n",
       " 'UBTF',\n",
       " 'OR1E1',\n",
       " 'MAK',\n",
       " 'KIAA0087',\n",
       " 'RAD9B',\n",
       " 'EEF1AKMT2',\n",
       " 'MFSD11',\n",
       " 'SMIM7',\n",
       " 'TRIML2',\n",
       " 'OR52A1',\n",
       " 'GPR84',\n",
       " 'HDGFL2',\n",
       " 'ERG',\n",
       " 'ADPGK',\n",
       " 'BACH2',\n",
       " 'PRAC2',\n",
       " 'TSHZ3',\n",
       " 'RGL3',\n",
       " 'LPAR4',\n",
       " 'FAM157A',\n",
       " 'FMN2',\n",
       " 'STMN1',\n",
       " 'ZNF460',\n",
       " 'RPS6KB2',\n",
       " 'TTLL11',\n",
       " 'CEP63',\n",
       " 'CFHR5',\n",
       " 'BABAM1',\n",
       " 'IL13',\n",
       " 'TIMM13',\n",
       " 'COPS7A',\n",
       " 'TRIP6',\n",
       " 'MBOAT2',\n",
       " 'DNMT3A',\n",
       " 'DDX53',\n",
       " 'FOXI2',\n",
       " 'CTDSPL2',\n",
       " 'ACADS',\n",
       " 'RLIM',\n",
       " 'AHSA1',\n",
       " 'PRKACA',\n",
       " 'HLA-DQB2',\n",
       " 'TP53TG5',\n",
       " 'RNF182',\n",
       " 'PCDHA5',\n",
       " 'ERI2',\n",
       " 'RRAGC',\n",
       " 'CTBS',\n",
       " 'MIA',\n",
       " 'FER',\n",
       " 'TCEAL2',\n",
       " 'GNPDA2',\n",
       " 'TAB3',\n",
       " 'SLC46A1',\n",
       " 'PLGLA',\n",
       " 'GPATCH2L',\n",
       " 'ACOT13',\n",
       " 'INIP',\n",
       " 'OR2T10',\n",
       " 'MAP3K6',\n",
       " 'CEP135',\n",
       " 'LYPD1',\n",
       " 'CCDC196',\n",
       " 'KMT5B',\n",
       " 'IKZF1',\n",
       " 'C2orf74',\n",
       " 'ARHGAP20',\n",
       " 'ZNF285',\n",
       " 'ANKRD36B',\n",
       " 'GGT5',\n",
       " 'OR5M11',\n",
       " 'DSN1',\n",
       " 'GGTA1',\n",
       " 'HADHB',\n",
       " 'FRYL',\n",
       " 'ESPNL',\n",
       " 'SLC10A6',\n",
       " 'PRSS55',\n",
       " 'KDELR1',\n",
       " 'SLC35F1',\n",
       " 'ZSCAN5C',\n",
       " 'SPDYE9',\n",
       " 'IFIT5',\n",
       " 'GPAA1',\n",
       " 'TMEM125',\n",
       " 'COPRS',\n",
       " 'GORASP2',\n",
       " 'ATOX1',\n",
       " 'C5AR1',\n",
       " 'IQCB1',\n",
       " 'LINC01551',\n",
       " 'ATL3',\n",
       " 'RIC8B',\n",
       " 'SDC2',\n",
       " 'VMO1',\n",
       " 'LINC01561',\n",
       " 'ZNF350',\n",
       " 'GART',\n",
       " 'SPAAR',\n",
       " 'C2orf80',\n",
       " 'DYDC1',\n",
       " 'GNAO1',\n",
       " 'EXOC3-AS1',\n",
       " 'CLCC1',\n",
       " 'ISG15',\n",
       " 'DIRC1',\n",
       " 'SPTLC1',\n",
       " 'PXMP4',\n",
       " 'SH3TC2',\n",
       " 'TAS2R5',\n",
       " 'GAGE2E',\n",
       " 'GNL3',\n",
       " 'CALHM4',\n",
       " 'RUFY1',\n",
       " 'ZNF107',\n",
       " 'ZBTB17',\n",
       " 'MTFR1L',\n",
       " 'DSC3',\n",
       " 'SEBOX',\n",
       " 'GRM2',\n",
       " 'MYCT1',\n",
       " 'FAM98A',\n",
       " 'RAB20',\n",
       " 'NEIL1',\n",
       " 'TMEM138',\n",
       " 'HVCN1',\n",
       " 'TRBV6-6',\n",
       " 'PI15',\n",
       " 'AGFG1',\n",
       " 'ZNF462',\n",
       " 'DUOXA2',\n",
       " 'ICAM2',\n",
       " 'TXNDC5',\n",
       " 'NDUFA6',\n",
       " 'ZIC5',\n",
       " 'ETV4',\n",
       " 'SIRT4',\n",
       " 'KRTAP9-7',\n",
       " 'MLX',\n",
       " 'CEP126',\n",
       " 'DEF6',\n",
       " 'OR3A2',\n",
       " 'POLR2J',\n",
       " 'IQCF1',\n",
       " 'ADAMTSL5',\n",
       " 'ENC1',\n",
       " 'PKP2',\n",
       " 'INPP5E',\n",
       " 'TOMT',\n",
       " 'RUVBL2',\n",
       " 'DNAJA2',\n",
       " 'PRSS36',\n",
       " 'ZSCAN23',\n",
       " 'NHLH2',\n",
       " 'RPL7L1',\n",
       " 'PRMT5',\n",
       " 'AGPAT2',\n",
       " 'XBP1',\n",
       " 'C1QTNF9B',\n",
       " 'MCRIP1',\n",
       " 'SLC5A3',\n",
       " 'KTN1-AS1',\n",
       " 'KCNS2',\n",
       " 'TMEM200C',\n",
       " 'PCDHB12',\n",
       " 'ATP10B',\n",
       " 'COX15',\n",
       " 'DLX3',\n",
       " 'SEMA3C',\n",
       " 'TRAPPC11',\n",
       " 'B3GNT3',\n",
       " 'PI4KAP2',\n",
       " 'C1QTNF5',\n",
       " 'NOTCH2NLC',\n",
       " 'TRIT1',\n",
       " 'ZMAT2',\n",
       " 'ZNF513',\n",
       " 'ARFGAP2',\n",
       " 'SMPD2',\n",
       " 'CFHR2',\n",
       " 'NKAPL',\n",
       " 'NSMCE1',\n",
       " 'OR5H6',\n",
       " 'SERPINB6',\n",
       " 'WBP1',\n",
       " 'CCDC103',\n",
       " 'SSU72L5',\n",
       " 'PPP1R2C',\n",
       " 'NCBP3',\n",
       " 'FSCN1',\n",
       " 'BCKDK',\n",
       " 'CYTH1',\n",
       " 'FUS',\n",
       " 'CTSH',\n",
       " 'OR2A2',\n",
       " 'RPS4Y1',\n",
       " 'LKAAEAR1',\n",
       " 'ACBD3',\n",
       " 'TRBV20-1',\n",
       " 'SMAD6',\n",
       " 'OR52N4',\n",
       " 'TMPRSS2',\n",
       " 'TAF11L5',\n",
       " 'MED10',\n",
       " 'THAP6',\n",
       " 'NPSR1',\n",
       " 'OR14I1',\n",
       " 'UTF1',\n",
       " 'NLRP5',\n",
       " 'CHRNE',\n",
       " 'STIM2',\n",
       " 'C10orf62',\n",
       " 'CCDC191',\n",
       " 'FAM90A26',\n",
       " 'MRPL39',\n",
       " 'CRMP1',\n",
       " 'MRAS',\n",
       " 'KIAA1191',\n",
       " 'ZNF782',\n",
       " 'GRM4',\n",
       " 'CRABP1',\n",
       " 'TRBV30',\n",
       " 'TNNI1',\n",
       " 'PKD2L1',\n",
       " 'ZNF420',\n",
       " 'ADAM30',\n",
       " 'PAM',\n",
       " 'FLVCR1',\n",
       " 'NCAPD3',\n",
       " 'U2AF2',\n",
       " 'PPP1R15B',\n",
       " 'ZNF804B',\n",
       " 'GUK1',\n",
       " 'MYOM1',\n",
       " 'SLC22A8',\n",
       " 'SLC52A1',\n",
       " 'NRP1',\n",
       " 'PRMT9',\n",
       " 'HES5',\n",
       " 'KCNK3',\n",
       " 'RIF1',\n",
       " 'GATA6',\n",
       " 'RBM10',\n",
       " 'DHFR2',\n",
       " 'FOXD1',\n",
       " 'TCHH',\n",
       " 'ARL8A',\n",
       " 'PALM3',\n",
       " 'BARD1',\n",
       " 'SS18L2',\n",
       " 'VLDLR',\n",
       " 'SPINT1',\n",
       " 'FSD1',\n",
       " 'TMEM177',\n",
       " 'RNF38',\n",
       " 'SSBP4',\n",
       " 'RUSC2',\n",
       " 'LXN',\n",
       " 'ITGA2B',\n",
       " 'G6PD',\n",
       " 'CCSER1',\n",
       " 'ZDHHC5',\n",
       " 'IL13RA1',\n",
       " 'CHD7',\n",
       " 'PTER',\n",
       " 'MLLT3',\n",
       " 'SMC2',\n",
       " 'LPO',\n",
       " 'PNO1',\n",
       " 'IGHV3-74',\n",
       " 'IFIT3',\n",
       " 'MECR',\n",
       " 'WDR54',\n",
       " 'CTU2',\n",
       " 'H1-9P',\n",
       " 'TRBV7-4',\n",
       " 'EIF5AL1',\n",
       " 'LSM5',\n",
       " 'SECISBP2L',\n",
       " 'TCF15',\n",
       " 'POFUT1',\n",
       " 'SLC30A9',\n",
       " 'STPG3',\n",
       " 'MRPL58',\n",
       " 'SNX12',\n",
       " 'WDHD1',\n",
       " 'G0S2',\n",
       " 'LINC02694',\n",
       " 'BCKDHA',\n",
       " 'TMEM71',\n",
       " 'BCS1L',\n",
       " 'CCDC40',\n",
       " 'DEFB131B',\n",
       " 'CD300E',\n",
       " 'HCN1',\n",
       " 'ZNF277',\n",
       " 'LTB4R2',\n",
       " 'ZNF493',\n",
       " 'PRSS51',\n",
       " 'ADM',\n",
       " 'RTN2',\n",
       " 'DSG2',\n",
       " 'H2BC3',\n",
       " 'OR1N2',\n",
       " 'COPS3',\n",
       " 'CHD9NB',\n",
       " 'KRT33A',\n",
       " 'CYP4F12',\n",
       " 'EDC3',\n",
       " 'RFPL1',\n",
       " 'WDR33',\n",
       " 'CYP2E1',\n",
       " 'GPR82',\n",
       " 'GPR161',\n",
       " 'CIZ1',\n",
       " 'ZDHHC17',\n",
       " 'RPS6KA5',\n",
       " 'AKTIP',\n",
       " 'CCDC112',\n",
       " 'KIF3C',\n",
       " 'ASF1A',\n",
       " 'CUL9',\n",
       " 'TP53RK',\n",
       " 'IPO11',\n",
       " 'KLHL7',\n",
       " 'STAC',\n",
       " 'ANXA2',\n",
       " 'INKA1',\n",
       " 'NHERF2',\n",
       " 'GNPTG',\n",
       " 'TMEM170A',\n",
       " 'TMPRSS7',\n",
       " 'MTBP',\n",
       " 'UTS2B',\n",
       " 'ERCC6L',\n",
       " 'ATP6V1C2',\n",
       " 'FNIP2',\n",
       " 'SLC35G6',\n",
       " 'C15orf62',\n",
       " 'TOLLIP',\n",
       " 'EBAG9',\n",
       " 'SPATA12',\n",
       " 'RAB1B',\n",
       " 'PAGE5',\n",
       " 'CCDC85B',\n",
       " 'TSHZ2',\n",
       " 'TMEM86B',\n",
       " 'CLTB',\n",
       " 'SLFNL1',\n",
       " 'GAS2L2',\n",
       " 'KCNB2',\n",
       " 'CDHR2',\n",
       " 'NGB',\n",
       " 'SV2A',\n",
       " 'ADNP2',\n",
       " 'ATP7B',\n",
       " 'FBP1',\n",
       " 'MIER2',\n",
       " 'CSAG2',\n",
       " 'NDUFS7',\n",
       " 'CHST10',\n",
       " 'RGSL1',\n",
       " 'GOLGB1',\n",
       " 'SPATA1',\n",
       " 'CFAP100',\n",
       " 'FAM200A',\n",
       " 'INTS13',\n",
       " 'ALPI',\n",
       " 'CCDC14',\n",
       " 'ITFG1',\n",
       " 'RTN4RL2',\n",
       " 'RAB23',\n",
       " 'CYP3A7',\n",
       " 'NAA60',\n",
       " 'ZDHHC1',\n",
       " 'UPF1',\n",
       " 'PCDHB6',\n",
       " 'ULK1',\n",
       " 'HIF1AN',\n",
       " 'TPSB2',\n",
       " 'TBC1D23',\n",
       " 'HEPACAM2',\n",
       " 'KIR3DX1',\n",
       " 'NUP54',\n",
       " 'AFF1',\n",
       " 'NUP153',\n",
       " 'TMEM25',\n",
       " 'B3GALT5',\n",
       " 'ABRACL',\n",
       " 'TDP1',\n",
       " 'IGFBP5',\n",
       " 'PRC1',\n",
       " 'TRBV16',\n",
       " 'MARCKSL1',\n",
       " 'DIP2B',\n",
       " 'MFGE8',\n",
       " 'IGHV1-69-2',\n",
       " 'DCAF15',\n",
       " 'OR6C3',\n",
       " 'RALBP1',\n",
       " 'CNKSR2',\n",
       " 'TRIM52',\n",
       " 'SEMG1',\n",
       " 'ADRM1',\n",
       " 'ACTRT3',\n",
       " 'EMILIN1',\n",
       " 'ZNF57',\n",
       " 'CLEC4G',\n",
       " 'RNF4',\n",
       " 'NAT10',\n",
       " 'HGH1',\n",
       " 'TERB2',\n",
       " 'SNX7',\n",
       " 'KDM2A',\n",
       " 'PKIG',\n",
       " 'TPM4',\n",
       " 'RBPJL',\n",
       " 'TRAJ3',\n",
       " 'USP3',\n",
       " 'ZNF98',\n",
       " 'IL18RAP',\n",
       " 'SESN1',\n",
       " 'FLVCR2',\n",
       " 'KYAT3',\n",
       " 'PUS10',\n",
       " 'TSPAN32',\n",
       " 'ALDH1B1',\n",
       " 'NFX1',\n",
       " 'NR1D2',\n",
       " 'PPP1CA',\n",
       " 'NRAP',\n",
       " 'PRELID3B',\n",
       " 'MYPN',\n",
       " 'STK10',\n",
       " 'ARSJ',\n",
       " 'SERPINB5',\n",
       " 'RNF13',\n",
       " 'OR1A2',\n",
       " 'COL4A2',\n",
       " 'GCM2',\n",
       " 'DGKE',\n",
       " 'PLPPR4',\n",
       " 'TGS1',\n",
       " 'ABHD17C',\n",
       " 'TMEFF2',\n",
       " 'KRT72',\n",
       " 'EGFL8',\n",
       " 'ADGRE1',\n",
       " 'FAM3A',\n",
       " 'GBP1',\n",
       " 'TEX55',\n",
       " 'MTSS1',\n",
       " 'C21orf140',\n",
       " 'BPY2',\n",
       " 'TRUB1',\n",
       " 'WDR3',\n",
       " 'MFSD2A',\n",
       " 'TSPAN14',\n",
       " 'CDKN3',\n",
       " 'DDX27',\n",
       " 'ANO1',\n",
       " 'ZPLD1',\n",
       " 'CECR2',\n",
       " 'PCSK4',\n",
       " 'OAZ2',\n",
       " 'USP27X',\n",
       " 'NDUFA5',\n",
       " 'MAPK4',\n",
       " 'FADD',\n",
       " 'INAFM2',\n",
       " 'CBY3',\n",
       " 'PKD1L2',\n",
       " 'CRIP2',\n",
       " 'RBM28',\n",
       " 'STAB1',\n",
       " 'C18orf32',\n",
       " 'C13orf42',\n",
       " 'PPP1R1B',\n",
       " 'TMEM141',\n",
       " 'AMY1B',\n",
       " 'NEMP2',\n",
       " 'MRGPRX2',\n",
       " 'TMPPE',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['gene_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032efb6-63f5-4c39-ba87-668de1ceac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = AnnDataTorchDataset(adata, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e639ce8-88f7-4b00-923e-81a9de46c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(torch_dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b673f-e21a-4f48-aa2b-481efc704b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch[''].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4a060-7686-404b-bfb6-6a7814115d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch['protein_embeddings'])#.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f4f1d-ce13-431e-9e46-dad13a57dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(embs.values.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f2fcf-a754-4cfa-a580-9d6df6c1f0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0b095-251e-4d76-afa4-d9711f8174fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad34d2-5524-4ae9-9fb1-5d4dba6063dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035e38fd-90c6-4701-ad84-7e55624b893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet = pd.read_parquet('/home/daniele/Code/scGraph/scgraph/protein_embeddings/human_embeddings.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe67ac-f3eb-4215-8389-965122de945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677931ec-7f68-4e27-a121-7a11d6bc0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet = parquet.loc[human_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ad097-14fa-4249-acc2-b35d7c7233c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ce1ed-eaf5-44a1-b8f1-f67f49071005",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata[:, list(human_dict.keys())].X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b1ec4-61cb-4ecd-953f-44b1bba1a155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6127600-6929-4e7f-ae25-e7c9e2285c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88323d9e-fe37-4eaa-a18a-5b03a291f8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76168d64-e49e-4315-9ca3-382fa2d318ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0589ff-8b01-4caf-a0ac-d4d4583da75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ca10f-756f-40f8-8fc0-e5fa00bf08f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb1d1f-a55f-4914-9f84-c00a621b58ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf3d1b-cb3a-4011-81c2-b0022424ad81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
